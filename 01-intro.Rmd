# Preamble {.unnumbered #index} 


This vignette explores predictive modeling and cluster analysis on
genomic scale data on colorectal cancer.  All data are available
on [NCBI GEO website](https://www.ncbi.nlm.nih.gov/geo/).

## License {-}

<!-- From https://github.com/santisoler/cc-licenses -->
<!-- THis doesnt work with pdf -->
<!-- COMMENT OUT FOR bookdown::pdf_book ????
![](CC_4_0.png)
![](https://i.creativecommons.org/l/by/4.0/88x31.png)

-->

`r knitr::include_graphics(
  "Static/images/CC_4_0.png",  dpi=100)`


This work by Francois Collin is licensed under a
[Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)



# Introduction {#intro}


Approximately 15 % of colorectal carcinomas (CRC) display high level microsatellite instability (MSI-H) due to either a germ line mutation in one of the genes responsible for DNA mismatch repair or somatic inactivation of the same pathway
[@Guinney:2015aa].  MSI is defined using the five primary microsatellite loci recommended at the 1997 National Cancer Institute-sponsored conference on MSI for the identification of MSI or replication errors in colorectal cancer [@Boland:1998aa]:

* 2 mononucleotide repeat markers: BAT25 and BAT26
* 3 dinucleotide repeat markers: D2S123, D5S346 and D17S250


Tumors are characterized as MSI-H if two or more of the five markers show instability (i.e., have insertion/deletion mutations) and MSI-L if only one of the five markers shows instability.  Note that the distinction between MSI-L and MSS can only be accomplished if a greater number of markers is utilized.  MSS and MSI CRC may have different prognoses and response to treatment.

## Outline

In this vignette we will download gene expression datasets from the Gene Expression Omnibus web site [GEO](https://www.ncbi.nlm.nih.gov/geo/) which have
MSI status as part of clinical sample characteristics and use these data to illustrate some gene expression data analysis steps:

- Data Preprocessing.
- Building a classifier to predict microsatellite instability status based on gene expression profiles.
- Discovering new sub-classes among CRC samples.


The main objectives of this vignette are:

- to demonstrate the use of R markdown to ensure reproducible research, and
- to demonstrate some of the capabilities of the `caret` R package.

The [knitr](https://yihui.name/knitr/) R package greatly
facilitates the use of R markdown to integrate data analysis and report writing
into a single process.  Used with
[bookdown[(https://bookdown.org/yihui/bookdown/) one can
easily write books and long-form articles from a series of R Markdown documents.


The [caret](https://topepo.github.io/caret/index.html) R package provides
a extensive set of tools for predictive modeling.  A very large selection
of modeling approaches can be invoked through a common interface and 
tools facilitate the process of implementing data splitting schemes to enable
hyper-parameter tuning through cross-validation and to produce reliable 
and comparable model performance estimates.

## Data Preprocessing

An important consideration when assembling a dataset for use in analyses aimed at answering scientific questions is how the data should pre-processed.  One of the main objectives of the pre-processing step is to remove technical variability without removing the biological variability of interest.  Some of the technical variability that affects microarray gene expression data are sample to sample variability due to differences in starting material quality and sample preparation effects.  These sources of variability are commonly handled by a pre-processing step known as *normalization*.  Even after normalization, some shared variability may be present.  This shared variability may be due to samples being processed at different time points, in different labs, using different instruments, etc.  These effects are commonly referred to as *batch effects*.  Options to remove batch effects include [@Lazar:2013aa]:

* Quantile normalization [@Bolstad:2003aa] (the baseline approach, which doesn't truly
address batch effects)
* RUV [@Gagnon-Bartsch:2012aa] and the more recent RUV-III [@Molania:2019aa]
* sva [@Leek:2012aa]

Selecting the method of normalization and batch correction that is most appropriate for a given problem requires careful consideration of the goals of an analysis.
It is helpful to follow proper experimental design practices when collecting the data including:

* processing the samples in a manner that avoids confounding batching and biological effects
* incorporating controls in the dataset, both positive and negative at both the sample and gene level

In this analysis we will simply use quantile normalization to re-normalize the
data after pooling across datasets and verify that egregious batch effects are not present.  We could subsequently repeat this analysis using a more sophisticated form of normalization and batch correction and assess the benefits that it brings to the analysis results.

## Affymetrix Probe Sets vs Genes

Affymetrix expression data are organized in probe sets which target genes [@Irizarry:2003aa].
There may be several probe sets for a given gene and the association between the two also depends
on which gene annotation is used (eg. HUGO vs Entrez).  Probe sets
which target a given gene sometimes yield discordant gene expression
estimates making reducing the probe set data to gene level estimates
somewhat problematic.  For these reasons, we will use the probe set as a gene
expression unit of analysis here. This should not affect classification
or clustering performance.  When it comes to the interpretation of a
particular predictive model, we can bring in the gene annotation associated
with each probe set at this point without loss of generality.

We should also note that some probe sets are known to cross-hybridize or
not hybridize specifically to the targeted gene's genomic sequence.  We will not
pre-filter genes based on this information in this analysis.


## Building a Classifier

Following pre-processing, the pooled dataset will be used to investigate the performance of
various classification methods to predict msi status from gene expression data.  The pooled dataset
will then be split into **Train** and **Test** subsets.  Following the split, the **Test** subset
is to be excluded from all analyses until we are ready to evaluate the final selection of predictive models
 under consideration.  During the analysis, the **Train** dataset will be further sub-divided into
**Fit** and **Validation** subsets for the purpose of model selection and hype-parameter tuning.
The Test set is not interrogated until the final model assessment step to ensure that it provides
a reliable set of data to assess the performance of the predictive models.
When the size of the dataset is small, it is
tempting to rely on cross-validated measures of performance instead of a  Test set.  We will compare the
two measures in this exercise to see if they lead to different orderings of the classifiers or drastically
different measures of performance.


When analyzing gene expression data for classification purposes, or to extract
biologically meaningful gene signatures, it is customary to first subset genes.
The primary reason for this subsetting step is for computational purposes -
including all of the genes in the analysis may require too much memory
or computing time for some steps.  A common approach used is to
apply a gene expression variability threshold for genes to be included in
the analysis - genes that show little variability across samples are
excluded.  Since our computing resources are limited, we will apply a fairly aggressive filter to the data.
We will reduce the data size by keeping only the 25% most variable genes for all subsequent analyses.
In practice, one would be more careful about applying this filter.

## Classifier Assessment

Many factors can be incorporated into classifier performance assessment, including:

* Predictive performance: Several metrics can be used to quantify predictive performance [@Vihinen:2012aa] and the choice is very much context dependent.
* Classifier cost: Some classifiers may achieve higher predictive performance but at a cost of requiring a more complete set of predictors or features.  This cost may be an important factor in some applications.
* Classifier interpretability: In some cases, the main purpose of building a classifier may be to get some insight into the biology at work.  Classifiers which implicitly or explicitly perform some sort of feature selection may provide an advantage in terms of interpretability over classifiers which rely on all features in manner that makes it difficult to determine feature importance.

For illustration purposes, we will report several measures of predictive performance.
We will also comment on each classifier's cost and interpretability.

## Subclass Discovery 

In some cases, one may be interested in analyzing gene expression data to uncover subgroups among the
sampled population.  This is one application of what it known as *cluster analysis*.
To evaluate the performance of cluster analysis methods for the purpose of
subclass discovery, we will treat the msi status as unknown and examine each method's ability to
recover this grouping in an unsupervised manner. 

We note in passing that in practice this application
requires particularly careful normalization and batch effect correction.   On the one hand, 
not removing systematic effects may lead samples to cluster in a way that will make uncovering
biological clusters very challenging.  On the other hand, the batch effect correction method may
erroneously remove biological effects in the data.  



The layout of the report is the following:  

* In Section \@ref(load-data) load the CRC data from GEO
and prepare the dataset for analysis.

* In Section \@ref(train-models) we fit some classification models.

* In Section \@ref(comp-models) we evaluate the various models.

* In Section \@ref(clustering) we perform some cluster analysis

* Concluding remarks are in Section \@ref(summary).

