# Train Predictive Models {#train-models}

We are ready to use the Training data to evaluate various predictive models.

Set up training parameters:
```{r train-models-setTrainParam, eval=T}
 suppressMessages(require(caret))

 SelGenes.CV <- paste(SelGenes, '.CV.', paste(unlist(CV), collapse='_'),sep='')

 cvControl <- trainControl(method = "repeatedcv", number=CV$Num, repeats=CV$Rep,
                           classProbs = TRUE, summaryFunction = twoClassSummary,
                           savePredictions='final')

 # Load Data
 loadObj(paste0('Train.',SelGenes,'.vec'), 'SelGenes.vec')
 
 load(file=file.path('RData', 'Train.Expr.mtx'))
 Train.SelGenes.Expr.mtx <- Train.Expr.mtx[, SelGenes.vec]
 rm(Train.Expr.mtx)

 load(file=file.path('RData', 'Train.Label.vec'))

 #dim(Train.SelGenes.Expr.mtx);length(Train.Label.vec)

``` 
<!-- ######################################################################## -->

```{r train-models-doMC, echo=F, eval=T}

 # Set up parallel computing.
 suppressMessages(require(doMC))
 suppressMessages(require(parallel))
 #cat("Cores =", detectCores(),'\n')
 registerDoMC(cores=detectCores())

```
<!-- ######################################################################## -->



We will use the `r SelGenes` gene set in this analysis.
Model tuning and optimization will be done based on
`r CV$Rep` repetitions of `r CV$Num`-fold cross-validations.
This provides `r CV$Rep` cross-validated, or out-of-sample, predicted values 
for each sample in the training set.  The distribution of predicted values
can be examined to identify hard to predict samples.  Some of these
samples may potentially be mislabelled, or may be hard to fit for other
reasons.  One might consider doing an analysis which excludes these samples
from the training set to see what impact they have on the fits.

Some of the models which can be evaluated with `caret` include:

* glmnet -  Lasso and Elastic-Net Regularized Generalized Linear Models
* knn - k nearest neighbors
* pam - Nearest shrunken centroids (see Tibshirani et al. (2002) [@Tibshirani:2002ab])
* svmRadial - Support vector machines (RBF kernel)
* gbm - Boosted trees
* xgbLinear - eXtreme Gradient Boosting
* xgbTree - eXtreme Gradient Boosting
* neuralnet - neural network  
* rf - Random forests
* stepLDA - Linear Discriminant Analysis with Stepwise Feature Selection
* stepQDA - Quadratic Discriminant Analysis with Stepwise Feature Selection

Many more [models](https://topepo.github.io/caret/available-models.html)
 can be implemented and evaluated with `caret`, including `deep learning` methods.  

<!-- 
Method requiring explicit dimensionality reduction:
* dlda, lda, qda - Classical linear disrimanent alalysis preceded  by variable selection

Methods which are not adapted to classification problems (penalized regression methods):
* pls - Partial least squares
* lasso - The lasso
* enet - Elastic net

These methods dont work.
* rda - Regularized discriminant analysis
* multinom - Logistic/Multinomial regression
* nnet - neural network
-->

## glmnet -  Elastic-Net Regularized Generalized Linear Models

In this run, we use the default search grid:  

* alpha: 0.1, 0.55, 1.00  

* lambda: 0.022, 0.071, 0.223


```{r train-models-glmnetFit, cache=TRUE,cache.vars='',echo=T, eval=T}
 ### CLEAR CACHE
 set.seed(12379)
 SelGenes.glmnetFit.tm <- system.time(
 SelGenes.glmnetFit <- train(Train.SelGenes.Expr.mtx, Train.Label.vec,
               method="glmnet", 
               trControl=cvControl, preProc=c('center','scale'))
 )
 print(SelGenes.glmnetFit.tm)
 print(SelGenes.glmnetFit)

 saveObj(paste0(SelGenes.CV, '.glmnetFit'), 'SelGenes.glmnetFit')

```
<!-- ######################################################################## -->


## enet -  Elastic-Net Regularized Generalized Linear Models

The glmnet fit selected a lasso fit with very few predictors.
In this run, we set alpha to 0.55 and vary lambda over
a grid ranging from 0.01 to 0.3.

```{r train-models-enetFit, cache=TRUE,cache.vars='',echo=T, eval=T}
 ### CLEAR CACHE
 set.seed(12379)
 SelGenes.enetFit.tm <- system.time(
 SelGenes.enetFit <- train(Train.SelGenes.Expr.mtx, Train.Label.vec,
               method="glmnet", 
               trControl=cvControl, preProc=c('center','scale'),
               tuneGrid = expand.grid(alpha = 0.55,
                                      lambda = seq(0.01,0.3,by = 0.02)))
 )
 print(SelGenes.enetFit.tm)
 print(SelGenes.enetFit)

 saveObj(paste0(SelGenes.CV, '.enetFit'), 'SelGenes.enetFit')

```
<!-- ######################################################################## -->


## knn - k nearest neighbors

```{r train-models-knnFit, cache=TRUE,cache.vars='', echo=T, eval=T}
 # CHANGE THIS LINE TO CLEAR CACHE
 ################################# 
 set.seed(12379)
 SelGenes.knnFit.tm <- system.time(
  SelGenes.knnFit <- train(Train.SelGenes.Expr.mtx, Train.Label.vec,
               method="knn", tuneLength=10,
               trControl=cvControl, preProc=c('center','scale'))
 )
 print(SelGenes.knnFit.tm)
 print(SelGenes.knnFit)

 saveObj(paste0(SelGenes.CV, '.knnFit'), 'SelGenes.knnFit')

```
<!-- ######################################################################## -->

## pam - nearest shrunken centroid

See Tibshirani et al. (2002) [@Tibshirani:2002ab] and
[web page](http://statweb.stanford.edu/~tibs/PAM/Rdist/howwork.html).

```{r train-models-pamFit, cache=TRUE,cache.vars='',echo=T, eval=T}
 set.seed(12379)
 SelGenes.pamFit.tm <- system.time(
 SelGenes.pamFit <- train(Train.SelGenes.Expr.mtx, Train.Label.vec,
               method="pam",
               trControl=cvControl, preProc=c('center','scale'))
 )
 print(SelGenes.pamFit.tm)
 print(SelGenes.pamFit)

 saveObj(paste0(SelGenes.CV, '.pamFit'), 'SelGenes.pamFit')

```
<!-- ######################################################################## -->


## svmRadial - Support vector machines (RBF kernel)
```{r train-models-svmRadialFit, cache=TRUE,cache.vars='',echo=T, eval=T}
 set.seed(12379)
 SelGenes.svmRadialFit.tm <- system.time(
 SelGenes.svmRadialFit <- train(Train.SelGenes.Expr.mtx, Train.Label.vec,
               method="svmRadial",
               trControl=cvControl, preProc=c('center','scale'))
 )
 print(SelGenes.svmRadialFit.tm)
 print(SelGenes.svmRadialFit)

 saveObj(paste0(SelGenes.CV, '.svmRadialFit'), 'SelGenes.svmRadialFit')

```
<!-- ######################################################################## -->

## gbm - Boosted trees
```{r train-models-gbmFit, cache=TRUE,cache.vars='',ecno=T, eval=T}
 set.seed(12379)
 SelGenes.gbmFit.tm <- system.time(
 SelGenes.gbmFit <- train(Train.SelGenes.Expr.mtx, Train.Label.vec,
               method="gbm", verbose=F,
               trControl=cvControl, preProc=c('center','scale'))
 )
 print(SelGenes.gbmFit.tm)
 print(SelGenes.gbmFit)

 saveObj(paste0(SelGenes.CV, '.gbmFit'), 'SelGenes.gbmFit')

```
<!-- ######################################################################## -->


## xgbLinear - eXtreme Gradient Boosting
```{r train-models-xgbLinearFit, cache=TRUE,cache.vars='',echo=T, eval=T}
 set.seed(12379)
 SelGenes.xgbLinearFit.tm <- system.time(
 SelGenes.xgbLinearFit <- train(Train.SelGenes.Expr.mtx, Train.Label.vec,
               method="xgbLinear", verbose=F,
               trControl=cvControl, preProc=c('center','scale'))
 )
 print(SelGenes.xgbLinearFit.tm)
 print(SelGenes.xgbLinearFit)

 saveObj(paste0(SelGenes.CV, '.xgbLinearFit'), 'SelGenes.xgbLinearFit')

```
<!-- ######################################################################## -->

<!-- TAKES TOOOO LOOOOONG 
## xgbTree - eXtreme Gradient Boosting
-->
```{r train-models-xgbTreeFit, cache=TRUE,cache.vars='',echo=FALSE, eval=FALSE}
 set.seed(12379)
 SelGenes.xgbTreeFit.tm <- system.time(
 SelGenes.xgbTreeFit <- train(Train.SelGenes.Expr.mtx, Train.Label.vec,
               method="xgbTree", verbose=F,
               trControl=cvControl, preProc=c('center','scale'))
 )
 print(SelGenes.xgbTreeFit.tm)
 print(SelGenes.xgbTreeFit)

 saveObj(paste0(SelGenes.CV, '.xgbTreeFit'), 'SelGenes.xgbTreeFit')

```
<!-- ######################################################################## -->


## rf - Random forests
```{r train-models-rfFit, cache=TRUE,cache.vars='',echo=T, eval=T}
 set.seed(12379)
 SelGenes.rfFit.tm <- system.time(
 SelGenes.rfFit <- train(Train.SelGenes.Expr.mtx, Train.Label.vec,
               method="rf",
               trControl=cvControl, preProc=c('center','scale'))
 )
 print(SelGenes.rfFit.tm)
 print(SelGenes.rfFit)

 saveObj(paste0(SelGenes.CV, '.rfFit'), 'SelGenes.rfFit')

```
<!-- ######################################################################## -->


## stepLDAFit - Stepwise linear discriminant analysis

```{r train-models-stepLDAFit, cache=TRUE,cache.vars='', echo=T, eval=T}
 # CHANGE THIS LINE TO CLEAR CACHE
 ################################# 
 set.seed(12379)
 SelGenes.stepLDAFit.tm <- system.time(
  SelGenes.stepLDAFit <- train(Train.SelGenes.Expr.mtx, Train.Label.vec,
               method="stepLDA",
               trControl=cvControl, preProc=c('center','scale'))
 )
 print(SelGenes.stepLDAFit.tm)

 print(SelGenes.stepLDAFit)

 saveObj(paste0(SelGenes.CV, '.stepLDAFit'), 'SelGenes.stepLDAFit')

``` 
<!-- ######################################################################## -->


<!--
## stepQDAFit - Stepwise quadratic discriminant analysis
-->
```{r train-models-stepQDAFit, cache=TRUE,cache.vars='', echo=F, eval=F}
 # CHANGE THIS LINE TO CLEAR CACHE
 ################################# 
 set.seed(12379)
 SelGenes.stepQDAFit.tm <- system.time(
  SelGenes.stepQDAFit <- train(Train.SelGenes.Expr.mtx, Train.Label.vec,
               method="stepQDA",
               trControl=cvControl, preProc=c('center','scale'))
 )
 print(SelGenes.stepQDAFit.tm)
 print(SelGenes.stepQDAFit)

 saveObj(paste0(SelGenes.CV, '.stepQDAFit'), 'SelGenes.stepQDAFit')

``` 
<!-- ######################################################################## -->

