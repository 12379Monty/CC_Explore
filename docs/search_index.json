[["index.html", "Classification and Clustering: Case Study with CRC Gene Expression Data Preamble License", " Classification and Clustering: Case Study with CRC Gene Expression Data Francois Collin 2020 Preamble This vignette explores predictive modeling and cluster analysis on genomic scale data on colorectal cancer. All data are available on NCBI GEO website. License This work by Francois Collin is licensed under a Creative Commons Attribution 4.0 International License "],["intro.html", "Section 1 Introduction 1.1 Outline 1.2 Data Preprocessing 1.3 Affymetrix Probe Sets vs Genes 1.4 Building a Classifier 1.5 Classifier Assessment 1.6 Subclass Discovery", " Section 1 Introduction Approximately 15 % of colorectal carcinomas (CRC) display high level microsatellite instability (MSI-H) due to either a germ line mutation in one of the genes responsible for DNA mismatch repair or somatic inactivation of the same pathway [1]. MSI is defined using the five primary microsatellite loci recommended at the 1997 National Cancer Institute-sponsored conference on MSI for the identification of MSI or replication errors in colorectal cancer [2]: 2 mononucleotide repeat markers: BAT25 and BAT26 3 dinucleotide repeat markers: D2S123, D5S346 and D17S250 Tumors are characterized as MSI-H if two or more of the five markers show instability (i.e., have insertion/deletion mutations) and MSI-L if only one of the five markers shows instability. Note that the distinction between MSI-L and MSS can only be accomplished if a greater number of markers is utilized. MSS and MSI CRC may have different prognoses and response to treatment. 1.1 Outline In this vignette we will download gene expression datasets from the Gene Expression Omnibus web site GEO which have MSI status as part of clinical sample characteristics and use these data to illustrate some gene expression data analysis steps: Data Preprocessing. Building a classifier to predict microsatellite instability status based on gene expression profiles. Discovering new sub-classes among CRC samples. The main objectives of this vignette are: to demonstrate the use of R markdown to ensure reproducible research, and to demonstrate some of the capabilities of the caret R package. The knitr R package greatly facilitates the use of R markdown to integrate data analysis and report writing into a single process. Used with [bookdown[(https://bookdown.org/yihui/bookdown/) one can easily write books and long-form articles from a series of R Markdown documents. The caret R package provides a extensive set of tools for predictive modeling. A very large selection of modeling approaches can be invoked through a common interface and tools facilitate the process of implementing data splitting schemes to enable hyper-parameter tuning through cross-validation and to produce reliable and comparable model performance estimates. 1.2 Data Preprocessing An important consideration when assembling a dataset for use in analyses aimed at answering scientific questions is how the data should pre-processed. One of the main objectives of the pre-processing step is to remove technical variability without removing the biological variability of interest. Some of the technical variability that affects microarray gene expression data are sample to sample variability due to differences in starting material quality and sample preparation effects. These sources of variability are commonly handled by a pre-processing step known as normalization. Even after normalization, some shared variability may be present. This shared variability may be due to samples being processed at different time points, in different labs, using different instruments, etc. These effects are commonly referred to as batch effects. Options to remove batch effects include [3]: Quantile normalization [4] (the baseline approach, which doesn’t truly address batch effects) RUV [5] and the more recent RUV-III [6] sva [7] Selecting the method of normalization and batch correction that is most appropriate for a given problem requires careful consideration of the goals of an analysis. It is helpful to follow proper experimental design practices when collecting the data including: processing the samples in a manner that avoids confounding batching and biological effects incorporating controls in the dataset, both positive and negative at both the sample and gene level In this analysis we will simply use quantile normalization to re-normalize the data after pooling across datasets and verify that egregious batch effects are not present. We could subsequently repeat this analysis using a more sophisticated form of normalization and batch correction and assess the benefits that it brings to the analysis results. 1.3 Affymetrix Probe Sets vs Genes Affymetrix expression data are organized in probe sets which target genes [8]. There may be several probe sets for a given gene and the association between the two also depends on which gene annotation is used (eg. HUGO vs Entrez). Probe sets which target a given gene sometimes yield discordant gene expression estimates making reducing the probe set data to gene level estimates somewhat problematic. For these reasons, we will use the probe set as a gene expression unit of analysis here. This should not affect classification or clustering performance. When it comes to the interpretation of a particular predictive model, we can bring in the gene annotation associated with each probe set at this point without loss of generality. We should also note that some probe sets are known to cross-hybridize or not hybridize specifically to the targeted gene’s genomic sequence. We will not pre-filter genes based on this information in this analysis. 1.4 Building a Classifier Following pre-processing, the pooled dataset will be used to investigate the performance of various classification methods to predict msi status from gene expression data. The pooled dataset will then be split into Train and Test subsets. Following the split, the Test subset is to be excluded from all analyses until we are ready to evaluate the final selection of predictive models under consideration. During the analysis, the Train dataset will be further sub-divided into Fit and Validation subsets for the purpose of model selection and hype-parameter tuning. The Test set is not interrogated until the final model assessment step to ensure that it provides a reliable set of data to assess the performance of the predictive models. When the size of the dataset is small, it is tempting to rely on cross-validated measures of performance instead of a Test set. To address the question of whether a test set is truly necesary, we will compare the two measures of performance to see if they lead to different orderings of the classifiers or drastically different measures of performance. When analyzing gene expression data for classification purposes, or to extract biologically meaningful gene signatures, it is customary to first subset genes. One reason for this subsetting step is for computational purposes - including all of the genes in the analysis may require too much memory or computing time for some steps. Another reason is to remove features that have no variability at all from the data set. A common approach used is to apply a gene expression variability threshold for genes to be included in the analysis - genes that show little variability across samples are excluded. For this analysis, we will reduce the data size by keeping the 30% most variable genes for all subsequent analyses. In practice, one might be more careful about applying this filter. 1.5 Classifier Assessment Many factors can be incorporated into classifier performance assessment, including: Predictive performance: Several metrics can be used to quantify predictive performance [9] and the choice is very much context dependent. Classifier cost: Some classifiers may achieve higher predictive performance but at a cost of requiring a more complete set of predictors or features. This cost may be an important factor in some applications. Classifier interpretability: In some cases, the main purpose of building a classifier may be to get some insight into the biology at work. Classifiers which implicitly or explicitly perform some sort of feature selection may provide an advantage in terms of interpretability over classifiers which rely on all features in manner that makes it difficult to determine feature importance. For illustration purposes, we will report several measures of predictive performance. We will also comment on each classifier’s cost and interpretability. 1.6 Subclass Discovery In some cases, one may be interested in analyzing gene expression data to uncover subgroups among the sampled population. This is one application of what it known as cluster analysis. To evaluate the performance of cluster analysis methods for the purpose of subclass discovery, we will treat the msi status as unknown and examine each method’s ability to recover this grouping in an unsupervised manner. We note in passing that in practice this application requires particularly careful normalization and batch effect correction. On the one hand, not removing systematic effects may lead samples to cluster in a way that will make uncovering biological clusters very challenging. On the other hand, the batch effect correction method may erroneously remove biological effects in the data. The layout of the report is the following: In Section 2 load the CRC data from GEO and prepare the dataset for analysis. In Section 3 we fit some classification models. In Section 4 we evaluate the various models. In Section 5 we perform some cluster analysis Concluding remarks are in Section 6. References "],["load-data.html", "Section 2 Load Data 2.1 Download Datasets 2.2 Peek into Datasets 2.3 Dataset Cleaning 2.4 Data Pooling 2.5 Batch Correction 2.6 Separate Data Set into Train and Test Subsets 2.7 Save Gene Sets", " Section 2 Load Data A search on the GEO web site1 identified the following datasets as potentialy useful for this exercise. All datasets have Affymetrix gene expression data for a number of samples annotated with msi status: GSE4554 [10]: [HG-U133_Plus_2] (84 CRC = 33 MSI + 51 MSS) GSE13067 [11]: [HG-U133_Plus_2] (74 CRC) GSE13294 [11]: [HG-U133_Plus_2] (89 MSI + 140 MSS + 58 MSI + 77 MSS) GSE24514 [12]: [HGU133A] (34 MSI,15 N) GSE30540 [13]: [HG-U133_Plus_2] (35 stage II and stage III) GSE35896 [14]: [HG-U133_Plus_2] (62 CRC samples) GSE75316 [15]: [HG-U133_Plus_2] (59 CRC samples) GSE39084 [16]: [HG-U133_Plus_2] (70 CRC samples) GSE26682 [17]: [HG-U133A], [HG-U133_Plus_2] (???) GSE14526 [18]: [HG-U133_Plus_2] GSE4045 [19]: [HG-U133A] 2.1 Download Datasets This process is somewhat manual as the GEO data do not follow an entirely standard format. Each dataset will have to be downloaded one at a time and examined. Following this step, we can loop again through the datasets and store the necessary data in a uniform manner. We can download data from GEO using function getGEO from package GEOquery. suppressMessages(require(GEOquery)) GSE_SET.vec &lt;- c(&#39;GSE4554&#39;, &#39;GSE13067&#39;, &#39;GSE13294&#39;, &#39;GSE24514&#39;, &#39;GSE30540&#39;, &#39;GSE35896&#39;, &#39;GSE75316&#39;, &#39;GSE39084&#39;, &#39;GSE26682&#39;, &#39;GSE14526&#39;, &#39;GSE4045&#39;) for(SET in GSE_SET.vec){ DnLd.tm &lt;- startTimedMessage(&quot;Start of Download for &quot;, SET) Set.gse &lt;- getGEO(SET, destdir=temp_DIR ,getGPL=F) saveObj(paste0(SET,&#39;.gse&#39;), &#39;Set.gse&#39;) stopTimedMessage(DnLd.tm) } 2.2 Peek into Datasets # CHANGE THIS LINE TO CLEAR CACHE suppressMessages(require(Biobase)) gseFile.vec &lt;- list.files(file.path(&#39;RData&#39;), &#39;gse$&#39;) gsePhenoData.lst &lt;- list() gseDim.lst &lt;- list() for(gseF in gseFile.vec){ cat(&quot;\\nPeek at &quot;, gseF, &#39;\\n&#39;) loadObj(gseF, &#39;Set.gse&#39;) gseName &lt;- sapply(strsplit(gseF, split=&#39;\\\\.&#39;),&#39;[&#39;,1) print(dim(exprs(Set.gse[[1]]))) if(nrow(exprs(Set.gse[[1]])) == 0) next() gseDim.lst[[gseName]] &lt;- dim(exprs(Set.gse[[1]])) ################################# # Expression Summary if(F) {#SKIP cat(&quot;\\nExpression Summary:\\n&quot;) knitr::kable(t(apply(exprs(Set.gse[[1]]),2,summary))) %&gt;% kableExtra::kable_styling(full_width = F) }@SKIP ################################# # Expression Boxplots old_par &lt;- par(mar=par(&#39;mar&#39;)+c(2,0,0,0)) boxplot(exprs(Set.gse[[1]]), outline=F, las=2) title(paste(&quot;Boxplots of expression values for&quot;, gseF)) par(old_par) ################################# gsePhenoData.lst[[gseName]] &lt;- phenoData(Set.gse[[1]])@data if(F) {#SKIP cat(&quot;\\nSample Descriptions:\\n&quot;) print(knitr::kable(phenoData(Set.gse[[1]])@data) %&gt;% kableExtra::kable_styling(full_width = F)) } }#SKIP ## ## Peek at GSE13067.gse ## [1] 54675 74 ## ## Peek at GSE13294.gse ## [1] 54675 155 ## ## Peek at GSE14526.gse ## [1] 0 8 ## ## Peek at GSE24514.gse ## [1] 22283 49 ## ## Peek at GSE26682.gse ## [1] 19473 176 ## ## Peek at GSE30540.gse ## [1] 54675 35 ## ## Peek at GSE35896.gse ## [1] 54675 62 ## ## Peek at GSE39084.gse ## [1] 54675 70 ## ## Peek at GSE4045.gse ## [1] 22283 37 ## ## Peek at GSE4554.gse ## [1] 54675 84 ## ## Peek at GSE75316.gse ## [1] 54675 59 2.3 Dataset Cleaning Following a peek into the datasets, we will proceed forward keeping only the datasets which used the HG-U133_Plus_2 gene chip (N=54675 probe sets) and have msi status: * GSE13067, GSE13294, GSE35896, GSE39084, GSE4554 We exclude GSE75316 from the analysis as most of the samples are already part of GSE13067 and the latter is a larger set. In the next chunk, we go through each of these datasets and store the gene expression and sample description data in a uniform manner to facilitate subsequent pooling. We also take a look at potential intra dataset batch effects. # CHANGE THIS LINE TO CLEAR CACHE .. suppressMessages(require(hgu133plus2.db)) for(SET in KEEP_GSE.vec){ cat(&quot;\\n\\nProcessing &quot;, SET, &#39;\\n&#39;) loadObj(paste0(SET, &#39;.gse&#39;), &#39;Set.gse&#39;) ################################################################### # Expr - matrix # - rows are features # - columns are samples ################################################################### Expr.mtx &lt;- exprs(Set.gse[[1]]) if(SET %in% c(&#39;GSE4554&#39;, &#39;GSE4045&#39;)) Expr.mtx &lt;- log2(pmax(Expr.mtx, min(Expr.mtx[Expr.mtx&gt;0]))) ################################################################### # SampAttr - data.frame # - rows are samples # - columns are atttibutes of the samples ################################################################### phenoData.frm &lt;- phenoData(Set.gse[[1]])@data if(nrow(phenoData.frm) != ncol(Expr.mtx)) stop(&quot;Expression-sampAttr Mismatch for &quot;, SET) MS_Status &lt;- switch(SET, GSE13067 = gsub(&#39; &#39;,&#39;&#39;, sapply(strsplit(phenoData.frm$title, split=&#39;:&#39;), function(x) x[2])), GSE13294 = gsub(&#39; &#39;,&#39;&#39;, sapply(strsplit(phenoData.frm$title, split=&#39;:&#39;), function(x) x[2])), GSE35896 = gsub(&#39; &#39;,&#39;&#39;, sapply(strsplit(phenoData.frm$characteristics_ch1.7, split=&#39;:&#39;), function(x) x[2])), GSE39084 = gsub(&#39; &#39;,&#39;&#39;, sapply(strsplit(phenoData.frm$characteristics_ch1.16, split=&#39;:&#39;), function(x) x[2])), GSE4554 = gsub(&#39; &#39;, &#39;&#39;, sapply(strsplit(as.character(phenoData.frm$description), split=&#39;,&#39;), function(x) x[1])), &quot;ERROR&quot;) KEEP.ndx &lt;- which(is.element(MS_Status, c(&#39;MSS&#39;, &#39;MSI&#39;, &#39;No&#39;, &#39;Low&#39;, &#39;High&#39;))) sampAttr.frm &lt;- data.frame(Sample_Id=rownames(phenoData.frm), MS_Status=MS_Status, row.names=rownames(phenoData.frm))[KEEP.ndx,] Expr.mtx &lt;- Expr.mtx[,rownames(sampAttr.frm)] print(table(MS_Status[KEEP.ndx])) ################################################################### # featureAttr - data.frame # - rows of featureAttr match rows of Expr mtx # - columns of featureAttr are atttibutes of the features - eg geneSymbol ################################################################### # This deosn;t work! #featureAttr.frm &lt;- featureData(Set.gse[[1]])@data #dim(featureAttr.frm) #Symbol.vec &lt;- mget(featureAttr.frm$ID, hgu133plus2SYMBOL) #featureAttr.frm$Symbol &lt;- Symbol.vec[rownames(featureAttr.frm)] Symbol.vec &lt;- mget(rownames(Expr.mtx), hgu133plus2SYMBOL) featureAttr.frm &lt;- data.frame(Symbol=unlist(Symbol.vec)) rownames(featureAttr.frm) &lt;- rownames(Expr.mtx) ################################################### # Save in arrayGeneExpr object ################################################### if(sum(colnames(Expr.mtx) != rownames(sampAttr.frm))) stop(&quot;Column order error&quot;) if(sum(rownames(Expr.mtx) != rownames(featureAttr.frm))) stop(&quot;Row order error&quot;) Set.arrayGeneExpr &lt;- list(exprData=NA, exprType=NA, Norm=NA, sampAttr=NA, featureAttr=NA) class(Set.arrayGeneExpr) &lt;- &quot;arrayGeneExpr&quot; Set.arrayGeneExpr$exprData &lt;- Expr.mtx Set.arrayGeneExpr$sampAttr &lt;- sampAttr.frm Set.arrayGeneExpr$featureAttr &lt;- featureAttr.frm saveObj(paste0(SET,&#39;.arrayGeneExpr&#39;), &#39;Set.arrayGeneExpr&#39;) ###################################################### # Look for intra-set batch effects ###################################################### Expt.prcomp &lt;- prcomp(Expr.mtx) plot(x=Expt.prcomp$rotation[,&#39;PC1&#39;], xlab=paste0(&#39;PC1 (&#39;, round(100*summary(Expt.prcomp)$importance[2,&quot;PC1&quot;],1),&#39;%)&#39;), y=Expt.prcomp$rotation[,&#39;PC2&#39;], ylab=paste0(&#39;PC2 (&#39;, round(100*summary(Expt.prcomp)$importance[2,&quot;PC2&quot;],1),&#39;%)&#39;), pch=19, cex=1.5, col=as.numeric(as.factor(sampAttr.frm$MS_Status))) title(SET) legend(ifelse(SET %in% c(&#39;GSE13067&#39;, &#39;GSE35896&#39;), &#39;bottomleft&#39;, &#39;bottomright&#39;), legend=levels(as.factor(sampAttr.frm$MS_Status)), col=1:length(levels(as.factor(sampAttr.frm$MS_Status))), pch=19) } ## ## ## Processing GSE13067 ## ## MSI MSS ## 11 63 ## ## ## Processing GSE13294 ## ## MSI MSS ## 78 77 ## ## ## Processing GSE35896 ## ## MSI MSS ## 5 56 ## ## ## Processing GSE39084 ## ## High Low No ## 16 3 51 ## ## ## Processing GSE4554 ## ## MSI MSS ## 33 50 2.4 Data Pooling When pooling data from different sources, we should make sure not to introduce batch effects into the mix. In the first instance we will simply pool the data sets together, and apply quantile normalization. This can be seen as a very weak form of batch effect correction and is a minimal requirement. For a more careful way to remove batch effects and other unwanted variation see Gagnon-Bartsch (2012) [5] and Molania et al. (2019) [6]. ############################### # Get feature attributes from one of the arrayGeneExpr objects ############################### loadObj(paste0(KEEP_GSE.vec[1],&#39;.arrayGeneExpr&#39;), &#39;Set.arrayGeneExpr&#39;) featureAttr.frm &lt;- Set.arrayGeneExpr$featureAttr featureAttr.frm$Symbol[is.na(featureAttr.frm$Symbol)] &lt;- rownames(featureAttr.frm)[is.na(featureAttr.frm$Symbol)] featureAttr.frm[1:5,,drop=F] ## Symbol ## 1007_s_at 1007_s_at ## 1053_at RFC2 ## 117_at HSPA6 ## 121_at PAX8 ## 1255_g_at GUCA1A geneId.vec &lt;- rownames(featureAttr.frm) ############################### ############################### # Pool the expression data ############################### PooledExpr.mtx &lt;- do.call(&#39;cbind&#39;, lapply(KEEP_GSE.vec, function(SET) { loadObj(paste0(SET,&#39;.arrayGeneExpr&#39;), &#39;Set.arrayGeneExpr&#39;) Set.arrayGeneExpr$exprData[geneId.vec,] })) #dim(PooledExpr.mtx) #PooledExpr.mtx[1:5, 1:5] # Apply quantile normalization median.Expr.vec &lt;- apply(apply(PooledExpr.mtx,2,sort), 1, median) #summary(median.Expr.vec) PooledNormedExpr.mtx &lt;- apply(PooledExpr.mtx, 2, function(x) median.Expr.vec[rank(x)]) rownames(PooledNormedExpr.mtx) &lt;- rownames(PooledExpr.mtx) #summary(apply(PooledExpr.mtx, 2, mean)) #summary(apply(PooledNormedExpr.mtx, 2, mean)) ############################### # Pool the sample attribute data ############################### PooledSampAttr.frm &lt;- do.call(&#39;rbind&#39;, lapply(KEEP_GSE.vec, function(SET) { #cat(SET,&#39;\\n&#39;) loadObj(paste0(SET,&#39;.arrayGeneExpr&#39;), &#39;Set.arrayGeneExpr&#39;) data.frame(GSE=rep(SET,nrow(Set.arrayGeneExpr$sampAttr)), Sample_Id=Set.arrayGeneExpr$sampAttr$Sample_Id, MS_Status=Set.arrayGeneExpr$sampAttr$MS_Status, row.names=rownames(Set.arrayGeneExpr$sampAttr)) })) #dim(PooledSampAttr.frm) #PooledSampAttr.frm[1:5, ] # Assemble arrayGeneExpr object for pooled data and save GSEPool.arrayGeneExpr &lt;- list(exprData=NA, exprType=NA, Norm=NA, sampAttr=NA, featureAttr=NA) class(GSEPool.arrayGeneExpr) &lt;- &quot;arrayGeneExpr&quot; GSEPool.arrayGeneExpr$exprData &lt;- PooledNormedExpr.mtx GSEPool.arrayGeneExpr$exprType &lt;- &#39;MAS5_RMA&#39; GSEPool.arrayGeneExpr$Norm &lt;- &#39;Quantile&#39; GSEPool.arrayGeneExpr$sampAttr &lt;- PooledSampAttr.frm GSEPool.arrayGeneExpr$featureAttr &lt;- featureAttr.frm save(GSEPool.arrayGeneExpr, file=file.path(&#39;RData&#39;,&#39;GSEPool.arrayGeneExpr&#39;)) loadObj(&#39;GSEPool.arrayGeneExpr&#39;, &#39;GSEPool.arrayGeneExpr&#39;) PooledNormedExpr.mtx &lt;- GSEPool.arrayGeneExpr$exprData ###################################################### # Look for inter-set batch effects ###################################################### Expt.prcomp &lt;- prcomp(PooledNormedExpr.mtx) par(mfrow=c(1,2), mar=c(3,3,2,1), oma=c(0,0,2,0)) # Annotate with Batch plot(x=Expt.prcomp$rotation[,&#39;PC1&#39;], xlab=paste0(&#39;PC1 (&#39;, round(100*summary(Expt.prcomp)$importance[2,&quot;PC1&quot;],1),&#39;%)&#39;), y=Expt.prcomp$rotation[,&#39;PC2&#39;], ylab=paste0(&#39;PC2 (&#39;, round(100*summary(Expt.prcomp)$importance[2,&quot;PC2&quot;],1),&#39;%)&#39;), pch=19, cex=1.5, col=as.numeric(as.factor(PooledSampAttr.frm$GSE))) title(&#39;Color is Batch&#39;) legend.tbl &lt;- table(as.factor(PooledSampAttr.frm$GSE), as.numeric(as.factor(PooledSampAttr.frm$GSE))) legend(&#39;topright&#39;, pch=19, cex=1.5, legend=rownames(legend.tbl), col=as.numeric(colnames(legend.tbl))) # Annotate with MS_Status MSS_MSI &lt;- as.factor(ifelse(is.element(PooledSampAttr.frm$MS_Status, c(&#39;MSS&#39;, &#39;No&#39;)), &#39;MSS&#39;,&#39;MSI&#39;)) plot(x=Expt.prcomp$rotation[,&#39;PC1&#39;], xlab=paste0(&#39;PC1 (&#39;, round(100*summary(Expt.prcomp)$importance[2,&quot;PC1&quot;],1),&#39;%)&#39;), y=Expt.prcomp$rotation[,&#39;PC2&#39;], ylab=paste0(&#39;PC2 (&#39;, round(100*summary(Expt.prcomp)$importance[2,&quot;PC2&quot;],1),&#39;%)&#39;), pch=19, cex=1.5, col=as.numeric(as.factor(MSS_MSI))) title(&#39;Color is MSS_MSI&#39;) legend.tbl &lt;- table(as.factor(MSS_MSI), as.numeric(as.factor(MSS_MSI))) legend(&#39;topright&#39;, pch=19, cex=1.5, legend=rownames(legend.tbl), col=as.numeric(colnames(legend.tbl))) mtext(side=3, outer=T, cex=1.25, &quot;Pooled GSE Data - Before Batch Correction&quot;) Figure 2.1: Pooled GSE Data - Before Batch Correction 2.5 Batch Correction We see a definite batch effect in the PCA plot based on the pooled expression data. The clustering is along probe set reduction method, RMA vs MAS 5.0. One should really go back to the cel files and re-analyze all samples using one probe set reduction method. For the sake of illustration, we will instead try to remove this effect using a batch-effect correction method. Given that we don’t have a good set of control genes as required by the RUV method, we’ll use ’ComBat` from the SVA package. Alternatively, we could keep keep the two sets of data, those summarized using RMA and those summarized using MAS 5.0, separate, alternating using one set for model selection and fitting and the other for testing. This would provide a strong test of the generalizability and robustness of the results. suppressMessages(require(sva)) load(file=file.path(&#39;RData&#39;,&#39;GSEPool.arrayGeneExpr&#39;)) Expr.mtx &lt;- GSEPool.arrayGeneExpr$exprData sampAttr.frm &lt;- GSEPool.arrayGeneExpr$sampAttr # Applying theComBatfunction to adjust for known batches # Note: we could here use a binray batch: RMA vs MAS5. batch &lt;- as.factor(sampAttr.frm$GSE) with(sampAttr.frm, table(GSE, MS_Status)) ## MS_Status ## GSE High Low MSI MSS No ## GSE13067 0 0 11 63 0 ## GSE13294 0 0 78 77 0 ## GSE35896 0 0 5 56 0 ## GSE39084 16 3 0 0 51 ## GSE4554 0 0 33 50 0 MSS &lt;- as.factor(is.element(sampAttr.frm$MS_Status, c(&#39;MSS&#39;, &#39;No&#39;))) modcombat = model.matrix(~ MSS) combat_edata = ComBat(dat=Expr.mtx, batch=batch, mod=modcombat, par.prior=TRUE, prior.plots=FALSE) ## Found5batches ## Adjusting for1covariate(s) or covariate level(s) ## Standardizing Data across genes ## Fitting L/S model and finding priors ## Finding parametric adjustments ## Adjusting the Data GSEPool.arrayGeneExpr$exprDataBatchAdj &lt;- combat_edata save(GSEPool.arrayGeneExpr, file=file.path(&#39;RData&#39;,&#39;GSEPool.arrayGeneExpr&#39;)) loadObj(&#39;GSEPool.arrayGeneExpr&#39;, &#39;GSEPool.arrayGeneExpr&#39;) ###################################################### # Look for inter-set batch effects ###################################################### Expt.prcomp &lt;- prcomp(GSEPool.arrayGeneExpr$exprDataBatchAdj) par(mfrow=c(1,2), mar=c(3,3,2,1), oma=c(0,0,2,0)) # Annotate with Batch plot(x=Expt.prcomp$rotation[,&#39;PC1&#39;], xlab=paste0(&#39;PC1 (&#39;, round(100*summary(Expt.prcomp)$importance[2,&quot;PC1&quot;],1),&#39;%)&#39;), y=Expt.prcomp$rotation[,&#39;PC2&#39;], ylab=paste0(&#39;PC2 (&#39;, round(100*summary(Expt.prcomp)$importance[2,&quot;PC2&quot;],1),&#39;%)&#39;), pch=19, cex=1.5, col=as.numeric(as.factor(sampAttr.frm$GSE))) title(&#39;Color is Batch&#39;) legend.tbl &lt;- table(as.factor(sampAttr.frm$GSE), as.numeric(as.factor(sampAttr.frm$GSE))) SKIP &lt;- function() { legend(&#39;topright&#39;, pch=19, cex=1.5, legend=rownames(legend.tbl), col=as.numeric(colnames(legend.tbl))) }#SKIP # Annotate with MS_Status MSS_MSI &lt;- as.factor(ifelse(is.element(sampAttr.frm$MS_Status, c(&#39;MSS&#39;, &#39;No&#39;)), &#39;MSS&#39;,&#39;MSI&#39;)) plot(x=Expt.prcomp$rotation[,&#39;PC1&#39;], xlab=paste0(&#39;PC1 (&#39;, round(100*summary(Expt.prcomp)$importance[2,&quot;PC1&quot;],1),&#39;%)&#39;), y=Expt.prcomp$rotation[,&#39;PC2&#39;], ylab=paste0(&#39;PC2 (&#39;, round(100*summary(Expt.prcomp)$importance[2,&quot;PC2&quot;],1),&#39;%)&#39;), pch=19, cex=1.5, col=as.numeric(as.factor(MSS_MSI))) title(&#39;Color is MSS_MSI&#39;) legend.tbl &lt;- table(as.factor(MSS_MSI), as.numeric(as.factor(MSS_MSI))) legend(&#39;topright&#39;, pch=19, cex=1.5, legend=rownames(legend.tbl), col=as.numeric(colnames(legend.tbl))) mtext(side=3, outer=T, cex=1.25, &quot;Pooled GSE Data - After Batch Correction&quot;) Figure 2.2: Pooled GSE Data - After Batch Correction Visual inspection of the effect of the batch correction transformation is a minimal requirement for verification. One can do a better job at quantifying the effect of batch correction, especially of the data set contains control features and control samples. See Lazar et.al. [3] for a discussion of batch effect removal assessment. 2.6 Separate Data Set into Train and Test Subsets We will separate the data set into Train and Test subsets here, before any other filtering or data manipulation. In particular, since we will be interested in evaluating the effect of gene or probe set selection, this selection must be made on the basis of the training subset only. This is done as a precautionary measure as gene selection based on a variability filter is is done here is unlikely to have an effect on sample classification performance. Take note that the expression matrices that we save here will be transposed with genes in columns. This is done to accomodate the data format expected by the caret package. # CHANGE THIS LINE TO CLEAR CACHE suppressMessages(require(caret)) # Load expression data object load(file.path(&#39;RData&#39;,&#39;GSEPool.arrayGeneExpr&#39;)) # Get expression data matrix Expr.mtx &lt;- t(GSEPool.arrayGeneExpr$exprDataBatchAdj) # Use gene names where possible colnames(Expr.mtx) &lt;- make.names(GSEPool.arrayGeneExpr$featureAttr[colnames(Expr.mtx),], unique=T) # Keep map GeneNameMap.vec &lt;- rownames(GSEPool.arrayGeneExpr$exprDataBatchAdj) names(GeneNameMap.vec) &lt;- colnames(Expr.mtx) #GeneNameMap.vec[1:5] save(GeneNameMap.vec, file=file.path(&#39;RData&#39;, &#39;GeneNameMap.vec&#39;)) # Get sample attributes sampAttr.frm &lt;- GSEPool.arrayGeneExpr$sampAttr DataSource.vec &lt;- sampAttr.frm$GSE names(DataSource.vec) &lt;- rownames(sampAttr.frm) Label.vec &lt;- ifelse(is.element(sampAttr.frm$MS_Status, c(&#39;MSS&#39;, &#39;No&#39;)), &#39;MSS&#39;,&#39;MSI&#39;) names(Label.vec) &lt;- rownames(sampAttr.frm) rm(GSEPool.arrayGeneExpr) # split Into Train and Test set.seed(12379) inTrain &lt;- createDataPartition(y=Label.vec, p=0.75, list=F) Train.Expr.mtx &lt;- Expr.mtx[inTrain,] Train.Label.vec &lt;- Label.vec[inTrain] Train.DataSource.vec &lt;- DataSource.vec[inTrain] Test.Expr.mtx &lt;- Expr.mtx[-inTrain,] Test.Label.vec &lt;- Label.vec[-inTrain] Test.DataSource.vec &lt;- Label.vec[-inTrain] knitr::kable(rbind(Train=dim(Train.Expr.mtx), Test=dim(Test.Expr.mtx))) %&gt;% kableExtra::kable_styling(full_width = F) Train 333 54675 Test 110 54675 knitr::kable(rbind( Train=table(Train.Label.vec)/length(Train.Label.vec), Test=table(Test.Label.vec)/length(Test.Label.vec))) %&gt;% kableExtra::kable_styling(full_width = F) MSI MSS Train 0.3303303 0.6696697 Test 0.3272727 0.6727273 # Save these save(Train.Expr.mtx, file=file.path(&#39;RData&#39;, &#39;Train.Expr.mtx&#39;)) save(Test.Expr.mtx, file=file.path(&#39;RData&#39;, &#39;Test.Expr.mtx&#39;)) save(Train.Label.vec, file=file.path(&#39;RData&#39;, &#39;Train.Label.vec&#39;)) save(Test.Label.vec, file=file.path(&#39;RData&#39;, &#39;Test.Label.vec&#39;)) save(Train.DataSource.vec, file=file.path(&#39;RData&#39;, &#39;Train.DataSource.vec&#39;)) save(Test.DataSource.vec, file=file.path(&#39;RData&#39;, &#39;Test.DataSource.vec&#39;)) 2.7 Save Gene Sets Save gene sets selected by overall variability in training samples. In the analysis that follows we will use the top 30% most variable genes. Note that we select genes based on variability in the entire training dataset which includes data from different GEO data sets. This selection may favor the inclusion of genes which differ across the different data sets due to technical reasons. An alternative selection would be to to select genes based on within data set variability. This could be implemented by filtering based on the residual variance of an ANOVA model Expr ~ GSE fitted to the gene expression data. # CHANGE THIS LINE TO CLEAR CACHE load(file.path(&#39;RData&#39;, &#39;Train.Expr.mtx&#39;)) # Identify and remove low variance columns Train.Expr.mad.vec &lt;- apply(Train.Expr.mtx,2,mad) #summary(Train.Expr.mad.vec) TopVar.cols &lt;- which(Train.Expr.mad.vec &gt; quantile(Train.Expr.mad.vec, prob=(100-VAR_FILTER)/100)) Train.TopVarGenes.vec &lt;- colnames(Train.Expr.mtx)[TopVar.cols] #length(Train.TopVarVarGenes.vec) saveObj(paste0(&#39;Train.&#39;,SelGenes,&#39;.vec&#39;), &#39;Train.TopVarGenes.vec&#39;) # also save top 25 and top 50 - Nawh! References "],["train-models.html", "Section 3 Train Predictive Models 3.1 glmnet - Lasso and Elastic-Net Regularized Generalized Linear Models 3.2 knn - k nearest neighbors 3.3 pam - nearest shrunken centroid 3.4 svmRadial - Support vector machines (RBF kernel) 3.5 gbm - Boosted trees 3.6 xgbLinear - eXtreme Gradient Boosting 3.7 rf - Random forests 3.8 stepLDAFit - Stepwise linear discriminant analysis", " Section 3 Train Predictive Models We are ready to use the Training data to evaluate various predictive models. Set up training parameters: suppressMessages(require(caret)) SelGenes.CV &lt;- paste(SelGenes, &#39;.CV.&#39;, paste(unlist(CV), collapse=&#39;_&#39;),sep=&#39;&#39;) cvControl &lt;- trainControl(method = &quot;repeatedcv&quot;, number=CV$Num, repeats=CV$Rep, classProbs = TRUE, summaryFunction = twoClassSummary, savePredictions=&#39;final&#39;) # Load Data loadObj(paste0(&#39;Train.&#39;,SelGenes,&#39;.vec&#39;), &#39;SelGenes.vec&#39;) load(file=file.path(&#39;RData&#39;, &#39;Train.Expr.mtx&#39;)) Train.SelGenes.Expr.mtx &lt;- Train.Expr.mtx[, SelGenes.vec] rm(Train.Expr.mtx) load(file=file.path(&#39;RData&#39;, &#39;Train.Label.vec&#39;)) #dim(Train.SelGenes.Expr.mtx);length(Train.Label.vec) We will use the Top30pVarGenes gene set in this analysis. Model tuning and optimization will be done based on 10 repetitions of 5-fold cross-validations. This provides 10 cross-validated, or out-of-sample, predicted values for each sample in the training set. The distribution of predicted values can be examined to identify hard to predict samples. Some of these samples may potentially be mislabelled, or may be hard to fit for other reasons. One might consider doing an analysis which excludes these samples from the training set to see what impact they have on the fits. Some of the models which can be evaluated with caret include: glmnet - Lasso and Elastic-Net Regularized Generalized Linear Models knn - k nearest neighbors pam - Nearest shrunken centroids (see Tibshirani et al. (2002) [20]) svmRadial - Support vector machines (RBF kernel) gbm - Boosted trees xgbLinear - eXtreme Gradient Boosting xgbTree - eXtreme Gradient Boosting neuralnet - neural network rf - Random forests stepLDA - Linear Discriminant Analysis with Stepwise Feature Selection stepQDA - Quadratic Discriminant Analysis with Stepwise Feature Selection Many more models can be implemented and evaluated with caret, including deep learning methods. 3.1 glmnet - Lasso and Elastic-Net Regularized Generalized Linear Models ### CLEAR CACHE set.seed(12379) SelGenes.glmnetFit.tm &lt;- system.time( SelGenes.glmnetFit &lt;- train(Train.SelGenes.Expr.mtx, Train.Label.vec, method=&quot;glmnet&quot;, trControl=cvControl, preProc=c(&#39;center&#39;,&#39;scale&#39;)) ) ## Warning in train.default(Train.SelGenes.Expr.mtx, Train.Label.vec, method = ## &quot;glmnet&quot;, : The metric &quot;Accuracy&quot; was not in the result set. ROC will be used ## instead. print(SelGenes.glmnetFit.tm) ## user system elapsed ## 912.616 39.970 92.403 print(SelGenes.glmnetFit) ## glmnet ## ## 333 samples ## 16403 predictors ## 2 classes: &#39;MSI&#39;, &#39;MSS&#39; ## ## Pre-processing: centered (16403), scaled (16403) ## Resampling: Cross-Validated (5 fold, repeated 10 times) ## Summary of sample sizes: 267, 267, 266, 266, 266, 266, ... ## Resampling results across tuning parameters: ## ## alpha lambda ROC Sens Spec ## 0.10 0.02236653 0.9464151 0.8363636 0.9892222 ## 0.10 0.07072917 0.9482107 0.8363636 0.9887778 ## 0.10 0.22366526 0.9519780 0.8445455 0.9878586 ## 0.55 0.02236653 0.9505358 0.8545455 0.9860808 ## 0.55 0.07072917 0.9536731 0.8581818 0.9874242 ## 0.55 0.22366526 0.9574412 0.7854545 0.9896667 ## 1.00 0.02236653 0.9521079 0.8572727 0.9838384 ## 1.00 0.07072917 0.9558131 0.8490909 0.9865354 ## 1.00 0.22366526 0.9583710 0.5609091 0.9950404 ## ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were alpha = 1 and lambda = 0.2236653. saveObj(paste0(SelGenes.CV, &#39;.glmnetFit&#39;), &#39;SelGenes.glmnetFit&#39;) 3.2 knn - k nearest neighbors # CHANGE THIS LINE TO CLEAR CACHE ################################# set.seed(12379) SelGenes.knnFit.tm &lt;- system.time( SelGenes.knnFit &lt;- train(Train.SelGenes.Expr.mtx, Train.Label.vec, method=&quot;knn&quot;, tuneLength=10, trControl=cvControl, preProc=c(&#39;center&#39;,&#39;scale&#39;)) ) ## Warning in train.default(Train.SelGenes.Expr.mtx, Train.Label.vec, method = ## &quot;knn&quot;, : The metric &quot;Accuracy&quot; was not in the result set. ROC will be used ## instead. print(SelGenes.knnFit.tm) ## user system elapsed ## 30947.824 945.229 5079.351 print(SelGenes.knnFit) ## k-Nearest Neighbors ## ## 333 samples ## 16403 predictors ## 2 classes: &#39;MSI&#39;, &#39;MSS&#39; ## ## Pre-processing: centered (16403), scaled (16403) ## Resampling: Cross-Validated (5 fold, repeated 10 times) ## Summary of sample sizes: 267, 267, 266, 266, 266, 266, ... ## Resampling results across tuning parameters: ## ## k ROC Sens Spec ## 5 0.7695179 0.2636364 0.9752828 ## 7 0.7958597 0.2245455 0.9887677 ## 9 0.8159293 0.2027273 0.9901212 ## 11 0.8207062 0.1863636 0.9945960 ## 13 0.8252415 0.1600000 0.9963838 ## 15 0.8206309 0.1454545 0.9977273 ## 17 0.8207498 0.1300000 0.9981919 ## 19 0.8360606 0.1136364 0.9977374 ## 21 0.8491770 0.1163636 0.9972828 ## 23 0.8553570 0.1190909 0.9963838 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was k = 23. saveObj(paste0(SelGenes.CV, &#39;.knnFit&#39;), &#39;SelGenes.knnFit&#39;) 3.3 pam - nearest shrunken centroid set.seed(12379) SelGenes.pamFit.tm &lt;- system.time( SelGenes.pamFit &lt;- train(Train.SelGenes.Expr.mtx, Train.Label.vec, method=&quot;pam&quot;, trControl=cvControl, preProc=c(&#39;center&#39;,&#39;scale&#39;)) ) ## 123456789101112131415161718192021222324252627282930 ## Warning in train.default(Train.SelGenes.Expr.mtx, Train.Label.vec, method = ## &quot;pam&quot;, : The metric &quot;Accuracy&quot; was not in the result set. ROC will be used ## instead. ## 1 print(SelGenes.pamFit.tm) ## user system elapsed ## 230.278 28.625 28.830 print(SelGenes.pamFit) ## Nearest Shrunken Centroids ## ## 333 samples ## 16403 predictors ## 2 classes: &#39;MSI&#39;, &#39;MSS&#39; ## ## Pre-processing: centered (16403), scaled (16403) ## Resampling: Cross-Validated (5 fold, repeated 10 times) ## Summary of sample sizes: 267, 267, 266, 266, 266, 266, ... ## Resampling results across tuning parameters: ## ## threshold ROC Sens Spec ## 0.3139646 0.8944164 0.8509091 0.8855758 ## 4.5524860 0.9558935 0.8718182 0.9811414 ## 8.7910075 0.5000000 0.0000000 1.0000000 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was threshold = 4.552486. saveObj(paste0(SelGenes.CV, &#39;.pamFit&#39;), &#39;SelGenes.pamFit&#39;) 3.4 svmRadial - Support vector machines (RBF kernel) set.seed(12379) SelGenes.svmRadialFit.tm &lt;- system.time( SelGenes.svmRadialFit &lt;- train(Train.SelGenes.Expr.mtx, Train.Label.vec, method=&quot;svmRadial&quot;, trControl=cvControl, preProc=c(&#39;center&#39;,&#39;scale&#39;)) ) ## Warning in train.default(Train.SelGenes.Expr.mtx, Train.Label.vec, method = ## &quot;svmRadial&quot;, : The metric &quot;Accuracy&quot; was not in the result set. ROC will be used ## instead. print(SelGenes.svmRadialFit.tm) ## user system elapsed ## 5304.564 164.344 501.062 print(SelGenes.svmRadialFit) ## Support Vector Machines with Radial Basis Function Kernel ## ## 333 samples ## 16403 predictors ## 2 classes: &#39;MSI&#39;, &#39;MSS&#39; ## ## Pre-processing: centered (16403), scaled (16403) ## Resampling: Cross-Validated (5 fold, repeated 10 times) ## Summary of sample sizes: 267, 267, 266, 266, 266, 266, ... ## Resampling results across tuning parameters: ## ## C ROC Sens Spec ## 0.25 0.9168315 0.8645455 0.8919091 ## 0.50 0.9168926 0.8663636 0.8892020 ## 1.00 0.9218545 0.8236364 0.9434040 ## ## Tuning parameter &#39;sigma&#39; was held constant at a value of 3.131629e-05 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were sigma = 3.131629e-05 and C = 1. saveObj(paste0(SelGenes.CV, &#39;.svmRadialFit&#39;), &#39;SelGenes.svmRadialFit&#39;) 3.5 gbm - Boosted trees set.seed(12379) SelGenes.gbmFit.tm &lt;- system.time( SelGenes.gbmFit &lt;- train(Train.SelGenes.Expr.mtx, Train.Label.vec, method=&quot;gbm&quot;, verbose=F, trControl=cvControl, preProc=c(&#39;center&#39;,&#39;scale&#39;)) ) ## Warning in train.default(Train.SelGenes.Expr.mtx, Train.Label.vec, method = ## &quot;gbm&quot;, : The metric &quot;Accuracy&quot; was not in the result set. ROC will be used ## instead. print(SelGenes.gbmFit.tm) ## user system elapsed ## 5643.175 64.762 642.978 print(SelGenes.gbmFit) ## Stochastic Gradient Boosting ## ## 333 samples ## 16403 predictors ## 2 classes: &#39;MSI&#39;, &#39;MSS&#39; ## ## Pre-processing: centered (16403), scaled (16403) ## Resampling: Cross-Validated (5 fold, repeated 10 times) ## Summary of sample sizes: 267, 267, 266, 266, 266, 266, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees ROC Sens Spec ## 1 50 0.9549440 0.8381818 0.9864949 ## 1 100 0.9545698 0.8445455 0.9874141 ## 1 150 0.9532507 0.8500000 0.9878687 ## 2 50 0.9543710 0.8390909 0.9865354 ## 2 100 0.9537709 0.8445455 0.9883232 ## 2 150 0.9545450 0.8463636 0.9878687 ## 3 50 0.9543113 0.8454545 0.9869899 ## 3 100 0.9538907 0.8354545 0.9887778 ## 3 150 0.9535262 0.8390909 0.9883131 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were n.trees = 50, interaction.depth = ## 1, shrinkage = 0.1 and n.minobsinnode = 10. saveObj(paste0(SelGenes.CV, &#39;.gbmFit&#39;), &#39;SelGenes.gbmFit&#39;) 3.6 xgbLinear - eXtreme Gradient Boosting set.seed(12379) SelGenes.xgbLinearFit.tm &lt;- system.time( SelGenes.xgbLinearFit &lt;- train(Train.SelGenes.Expr.mtx, Train.Label.vec, method=&quot;xgbLinear&quot;, verbose=F, trControl=cvControl, preProc=c(&#39;center&#39;,&#39;scale&#39;)) ) ## Warning in train.default(Train.SelGenes.Expr.mtx, Train.Label.vec, method = ## &quot;xgbLinear&quot;, : The metric &quot;Accuracy&quot; was not in the result set. ROC will be used ## instead. print(SelGenes.xgbLinearFit.tm) ## user system elapsed ## 49679.076 238.865 28470.811 print(SelGenes.xgbLinearFit) ## eXtreme Gradient Boosting ## ## 333 samples ## 16403 predictors ## 2 classes: &#39;MSI&#39;, &#39;MSS&#39; ## ## Pre-processing: centered (16403), scaled (16403) ## Resampling: Cross-Validated (5 fold, repeated 10 times) ## Summary of sample sizes: 267, 267, 266, 266, 266, 266, ... ## Resampling results across tuning parameters: ## ## lambda alpha nrounds ROC Sens Spec ## 0e+00 0e+00 50 0.9497185 0.8572727 0.9735253 ## 0e+00 0e+00 100 0.9497185 0.8572727 0.9735253 ## 0e+00 0e+00 150 0.9497185 0.8572727 0.9735253 ## 0e+00 1e-04 50 0.9502571 0.8554545 0.9735152 ## 0e+00 1e-04 100 0.9502571 0.8554545 0.9735152 ## 0e+00 1e-04 150 0.9502571 0.8554545 0.9735152 ## 0e+00 1e-01 50 0.9489688 0.8590909 0.9762121 ## 0e+00 1e-01 100 0.9489688 0.8590909 0.9762121 ## 0e+00 1e-01 150 0.9489688 0.8590909 0.9762121 ## 1e-04 0e+00 50 0.9481827 0.8527273 0.9735152 ## 1e-04 0e+00 100 0.9481827 0.8527273 0.9735152 ## 1e-04 0e+00 150 0.9481827 0.8527273 0.9735152 ## 1e-04 1e-04 50 0.9481371 0.8581818 0.9735051 ## 1e-04 1e-04 100 0.9481371 0.8581818 0.9735051 ## 1e-04 1e-04 150 0.9481371 0.8581818 0.9735051 ## 1e-04 1e-01 50 0.9487888 0.8600000 0.9757677 ## 1e-04 1e-01 100 0.9487888 0.8600000 0.9757677 ## 1e-04 1e-01 150 0.9487888 0.8600000 0.9757677 ## 1e-01 0e+00 50 0.9482635 0.8627273 0.9771111 ## 1e-01 0e+00 100 0.9482635 0.8627273 0.9771111 ## 1e-01 0e+00 150 0.9482635 0.8627273 0.9771111 ## 1e-01 1e-04 50 0.9481189 0.8627273 0.9771111 ## 1e-01 1e-04 100 0.9481189 0.8627273 0.9771111 ## 1e-01 1e-04 150 0.9481189 0.8627273 0.9771111 ## 1e-01 1e-01 50 0.9484256 0.8527273 0.9762222 ## 1e-01 1e-01 100 0.9484256 0.8527273 0.9762222 ## 1e-01 1e-01 150 0.9484256 0.8527273 0.9762222 ## ## Tuning parameter &#39;eta&#39; was held constant at a value of 0.3 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were nrounds = 50, lambda = 0, alpha ## = 1e-04 and eta = 0.3. saveObj(paste0(SelGenes.CV, &#39;.xgbLinearFit&#39;), &#39;SelGenes.xgbLinearFit&#39;) 3.7 rf - Random forests set.seed(12379) SelGenes.rfFit.tm &lt;- system.time( SelGenes.rfFit &lt;- train(Train.SelGenes.Expr.mtx, Train.Label.vec, method=&quot;rf&quot;, trControl=cvControl, preProc=c(&#39;center&#39;,&#39;scale&#39;)) ) ## Warning in train.default(Train.SelGenes.Expr.mtx, Train.Label.vec, method ## = &quot;rf&quot;, : The metric &quot;Accuracy&quot; was not in the result set. ROC will be used ## instead. print(SelGenes.rfFit.tm) ## user system elapsed ## 50392.709 122.806 18379.771 print(SelGenes.rfFit) ## Random Forest ## ## 333 samples ## 16403 predictors ## 2 classes: &#39;MSI&#39;, &#39;MSS&#39; ## ## Pre-processing: centered (16403), scaled (16403) ## Resampling: Cross-Validated (5 fold, repeated 10 times) ## Summary of sample sizes: 267, 267, 266, 266, 266, 266, ... ## Resampling results across tuning parameters: ## ## mtry ROC Sens Spec ## 2 0.9103446 0.2090909 0.9991111 ## 181 0.9554389 0.8172727 0.9856162 ## 16402 0.9519348 0.8327273 0.9721818 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 181. saveObj(paste0(SelGenes.CV, &#39;.rfFit&#39;), &#39;SelGenes.rfFit&#39;) 3.8 stepLDAFit - Stepwise linear discriminant analysis # CHANGE THIS LINE TO CLEAR CACHE ################################# set.seed(12379) SelGenes.stepLDAFit.tm &lt;- system.time( SelGenes.stepLDAFit &lt;- train(Train.SelGenes.Expr.mtx, Train.Label.vec, method=&quot;stepLDA&quot;, trControl=cvControl, preProc=c(&#39;center&#39;,&#39;scale&#39;)) ) ## Warning in train.default(Train.SelGenes.Expr.mtx, Train.Label.vec, method = ## &quot;stepLDA&quot;, : The metric &quot;Accuracy&quot; was not in the result set. ROC will be used ## instead. ## `stepwise classification&#39;, using 10-fold cross-validated correctness rate of method lda&#39;. ## 333 observations of 16403 variables in 2 classes; direction: both ## stop criterion: improvement less than 5%. ## correctness rate: 0.9098; in: &quot;CBX5.2&quot;; variables (1): CBX5.2 ## ## hr.elapsed min.elapsed sec.elapsed ## 0.00 8.00 42.09 print(SelGenes.stepLDAFit.tm) ## user system elapsed ## 61405.643 236.295 24023.705 print(SelGenes.stepLDAFit) ## Linear Discriminant Analysis with Stepwise Feature Selection ## ## 333 samples ## 16403 predictors ## 2 classes: &#39;MSI&#39;, &#39;MSS&#39; ## ## Pre-processing: centered (16403), scaled (16403) ## Resampling: Cross-Validated (5 fold, repeated 10 times) ## Summary of sample sizes: 267, 267, 266, 266, 266, 266, ... ## Resampling results: ## ## ROC Sens Spec ## 0.9258485 0.7554545 0.9610101 ## ## Tuning parameter &#39;maxvar&#39; was held constant at a value of Inf ## Tuning ## parameter &#39;direction&#39; was held constant at a value of both saveObj(paste0(SelGenes.CV, &#39;.stepLDAFit&#39;), &#39;SelGenes.stepLDAFit&#39;) References "],["comp-models.html", "Section 4 Compare Predictive Models 4.1 Load Models 4.2 Compare Times 4.3 Compare Prediction Accuracy 4.4 Compare Models in Terms of ROC 4.5 Look at Variable Importance 4.6 Classification: Discussion", " Section 4 Compare Predictive Models Here we compare predictive models in terms of: Processing time Prediction accuracy and ROC on train samples Prediction accuracy and ROC on train out-of-sample Prediction accuracy and ROC on test samples Direct comparison of variable importance 4.1 Load Models ### BREAK CACHE # Load Train and Test Data load(file=file.path(&#39;RData&#39;, &#39;Train.Expr.mtx&#39;)) load(file=file.path(&#39;RData&#39;, &#39;Test.Expr.mtx&#39;)) load(file=file.path(&#39;RData&#39;, &#39;Train.Label.vec&#39;)) load(file=file.path(&#39;RData&#39;, &#39;Test.Label.vec&#39;)) CLASS1 &lt;- as.character(sort(unique(Train.Label.vec))[1]) CLASS2 &lt;- as.character(sort(unique(Train.Label.vec))[2]) # Load models (Only CV.5_10?) ModelFit.vec &lt;- list.files(file.path(&#39;RData&#39;), &#39;Fit$&#39;) FitSetCV.vec &lt;- sapply(strsplit(ModelFit.vec, split=&#39;\\\\.&#39;), function(x) paste(x[1:3], collapse=&#39;.&#39;)) ModelFit.lst &lt;- split(sapply(strsplit(ModelFit.vec, split=&#39;\\\\.&#39;), function(x) rev(x)[1]), FitSetCV.vec) # Get Model.col for plotting (only have one FitSetCV values here. could have many) Model.col &lt;- as.numeric(as.factor(ModelFit.lst[[1]])) names(Model.col) &lt;- ModelFit.lst[[1]] cat(&quot;Found&quot;, length(ModelFit.vec), &#39;models:\\n&#39;) ## Found 9 models: print(ModelFit.lst) ## $Top30pVarGenes.CV.5_10 ## [1] &quot;gbmFit&quot; &quot;glmnetFit&quot; &quot;knnFit&quot; &quot;pamFit&quot; &quot;rfFit&quot; ## [6] &quot;stepLDAFit&quot; &quot;svmRadialFit&quot; &quot;xgbLinearFit&quot; ## ## $Top60VarGenes.CV.10_30 ## [1] &quot;glmnetFit&quot; # Load Models for single set (we cd have more than one set in FitSetCV.vec) SET &lt;- names(ModelFit.lst)[1] Set.ModelFit.lst &lt;- lapply(ModelFit.lst[[SET]], function(MF) { loadObj(paste(SET, MF, sep=&#39;.&#39;), &#39;ModelFit&#39;) ModelFit}) names(Set.ModelFit.lst) &lt;- ModelFit.lst[[SET]] 4.2 Compare Times ### BREAK CACHE Set.ModelTimes.frm &lt;- do.call(&#39;rbind&#39;, lapply(Set.ModelFit.lst, function(LL) c(all=LL$times$everything)))###, final=LL$times$final))) knitr::kable(t(Set.ModelTimes.frm), digits=2, caption=&quot;Compute times&quot;) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.1: Compute times gbmFit glmnetFit knnFit pamFit rfFit stepLDAFit svmRadialFit xgbLinearFit all.user.self 13.32 7.30 2.81 5.00 96.90 522.52 19.22 17.67 all.sys.self 0.49 0.42 1.71 0.52 0.77 3.37 1.63 0.53 all.elapsed 642.98 92.40 5079.35 28.83 18379.77 24023.71 501.06 28470.81 all.user.child 5629.85 905.31 30945.02 225.27 50295.81 60883.12 5285.34 49661.41 all.sys.child 64.28 39.55 943.51 28.11 122.04 232.92 162.72 238.33 4.3 Compare Prediction Accuracy 4.3.1 Train Data Accuracy ### BREAK CACHE suppressMessages(require(caret)) # Train - these are fitting errors Set.Train.Pred.lst &lt;- suppressMessages(predict(Set.ModelFit.lst)) #, newdata=Train.SelGenes.Expr.mtx)) Set.Train.TruthTable.frm &lt;- do.call(&#39;rbind&#39;, lapply(Set.Train.Pred.lst, function(PRED) { truth.vec &lt;- as.vector(table(PRED, Train.Label.vec))#/length(Train.Label.vec) # ASSUMING MSI is FIRST LABEL of CLASS names(truth.vec) &lt;- c(&#39;TN&#39;, &#39;FP&#39;, &#39;FN&#39;, &#39;TP&#39;) truth.vec})) knitr::kable(data.frame(cbind(Set.Train.TruthTable.frm, Set.Train.TruthTable.frm/length(Train.Label.vec))), digits=2, align=&#39;c&#39;, caption=&#39;Model CV Accuracy&#39;) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.2: Model CV Accuracy TN FP FN TP TN.1 FP.1 FN.1 TP.1 gbmFit 103 7 1 222 0.31 0.02 0.00 0.67 glmnetFit 66 44 1 222 0.20 0.13 0.00 0.67 knnFit 14 96 0 223 0.04 0.29 0.00 0.67 pamFit 110 0 223 0 0.33 0.00 0.67 0.00 rfFit 110 0 0 223 0.33 0.00 0.00 0.67 stepLDAFit 87 23 7 216 0.26 0.07 0.02 0.65 svmRadialFit 103 7 1 222 0.31 0.02 0.00 0.67 xgbLinearFit 110 0 0 223 0.33 0.00 0.00 0.67 4.3.2 Out of Sample Train Data Accuracy ### BREAK CACHE Set.ModelFit.osPred.mtx.lst &lt;- lapply(Set.ModelFit.lst, function(MF) { MF.pred.mtx &lt;- MF$pred Rep.vec &lt;- sapply(strsplit(MF$pred$Resample, split=&#39;\\\\.&#39;),&#39;[&#39;, 2) rowIndex.vec &lt;- sort(unique(MF$pred$rowIndex)) Pred.mtx &lt;- do.call(&#39;cbind&#39;, lapply(split(MF$pred, Rep.vec), function(RepMFpred.frm) as.character(RepMFpred.frm[match(rowIndex.vec, RepMFpred.frm$rowIndex),&#39;pred&#39;]))) Pred.mtx}) # Use mode over reps as prediction ############################################ Set.ModelFit.osPred.vec.lst &lt;- lapply(Set.ModelFit.osPred.mtx.lst, function(PRED.mtx) apply(PRED.mtx,1,function(Pred.vec) names(table(Pred.vec))[which.max(table(Pred.vec))])) Set.Train.osPredMode.TruthTable.frm &lt;- do.call(&#39;rbind&#39;, lapply(Set.ModelFit.osPred.vec.lst, function(PRED) { truth.vec &lt;- as.vector(table(PRED, Train.Label.vec))#/length(Train.Label.vec) # ASSUMING MSI is FIRST LABEL of CLASS names(truth.vec) &lt;- c(&#39;TN&#39;, &#39;FP&#39;, &#39;FN&#39;, &#39;TP&#39;) truth.vec})) knitr::kable(data.frame(cbind(Set.Train.osPredMode.TruthTable.frm, Set.Train.osPredMode.TruthTable.frm/length(Train.Label.vec))), digits=2, align=&#39;c&#39;, caption=&#39;Model Out-of-Sample Accuracy - Mode&#39;) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.3: Model Out-of-Sample Accuracy - Mode TN FP FN TP TN.1 FP.1 FN.1 TP.1 gbmFit 93 17 3 220 0.28 0.05 0.01 0.66 glmnetFit 62 48 1 222 0.19 0.14 0.00 0.67 knnFit 13 97 1 222 0.04 0.29 0.00 0.67 pamFit 96 14 4 219 0.29 0.04 0.01 0.66 rfFit 90 20 3 220 0.27 0.06 0.01 0.66 stepLDAFit 87 23 8 215 0.26 0.07 0.02 0.65 svmRadialFit 92 18 12 211 0.28 0.05 0.04 0.63 xgbLinearFit 95 15 5 218 0.29 0.05 0.02 0.65 ############################################ # alternatively, can look at the osPred by rep # and use mean for error rates ############################################ Set.Train.osPredMean.TruthTable.frm &lt;- do.call(&#39;rbind&#39;, lapply(Set.ModelFit.osPred.mtx.lst, function(PRED.mtx) { TruthTable.mtx &lt;- do.call(&#39;rbind&#39;, lapply(1:ncol(PRED.mtx), function(CC) as.vector(table(PRED.mtx[,CC], Train.Label.vec)/length(Train.Label.vec)))) apply(TruthTable.mtx,2,mean)})) colnames(Set.Train.osPredMean.TruthTable.frm) &lt;- c(&#39;TN&#39;, &#39;FP&#39;, &#39;FN&#39;, &#39;TP&#39;) knitr::kable(data.frame(Set.Train.osPredMean.TruthTable.frm), digits=2, align=&#39;c&#39;, caption=&#39;Model Out-of-Sample Accuracy - MeanPred&#39;) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.3: Model Out-of-Sample Accuracy - MeanPred TN FP FN TP gbmFit 0.28 0.05 0.01 0.66 glmnetFit 0.19 0.15 0.00 0.67 knnFit 0.04 0.29 0.00 0.67 pamFit 0.29 0.04 0.01 0.66 rfFit 0.27 0.06 0.01 0.66 stepLDAFit 0.25 0.08 0.03 0.64 svmRadialFit 0.27 0.06 0.04 0.63 xgbLinearFit 0.28 0.05 0.02 0.65 4.3.3 Test Data Accuracy ### BREAK CACHE suppressMessages(require(caret)) # Test - these are fitting errors Set.Test.Pred.lst &lt;- lapply(Set.ModelFit.lst, function(MF) predict(MF, newdata=Test.Expr.mtx)) ## Warning in method$predict(modelFit = modelFit, newdata = newdata, submodels = ## param): kernlab class prediction calculations failed; returning NAs Set.Test.TruthTable.frm &lt;- do.call(&#39;rbind&#39;, lapply(Set.Test.Pred.lst, function(PRED) { truth.vec &lt;- as.vector(table(PRED, Test.Label.vec))#/length(Test.Label.vec) # ASSUMING MSI is FIRST LABEL of CLASS names(truth.vec) &lt;- c(&#39;TN&#39;, &#39;FP&#39;, &#39;FN&#39;, &#39;TP&#39;) truth.vec})) knitr::kable(data.frame(cbind(Set.Test.TruthTable.frm, Set.Test.TruthTable.frm/length(Test.Label.vec))), digits=2, align=&#39;c&#39;, caption=&#39;Model Test Set Accuracy&#39;) %&gt;% kableExtra::kable_styling(full_width = F) Table 4.4: Model Test Set Accuracy TN FP FN TP TN.1 FP.1 FN.1 TP.1 gbmFit 32 4 2 72 0.29 0.04 0.02 0.65 glmnetFit 21 15 1 73 0.19 0.14 0.01 0.66 knnFit 5 31 0 74 0.05 0.28 0.00 0.67 pamFit 34 2 4 70 0.31 0.02 0.04 0.64 rfFit 32 4 1 73 0.29 0.04 0.01 0.66 stepLDAFit 30 6 7 67 0.27 0.05 0.06 0.61 svmRadialFit 0 0 0 0 0.00 0.00 0.00 0.00 xgbLinearFit 31 5 4 70 0.28 0.05 0.04 0.64 4.4 Compare Models in Terms of ROC 4.4.1 Training Data ROC ### BREAK CACHE # CHANGE THIS LINE TO CLEAR CACHE suppressMessages(require(pROC)) ################################ # Train ################################ # Get predicted probabilities Set.Train.Prob.lst &lt;- suppressMessages(predict(Set.ModelFit.lst, type=&#39;prob&#39;)) Set.Train.ProbClass1.mtx &lt;- do.call(&#39;cbind&#39;, lapply(Set.Train.Prob.lst, function(x) x[,CLASS1])) rownames(Set.Train.ProbClass1.mtx) &lt;- names(Train.Label.vec) # ROC Set.Train.roc.mtx.lst &lt;- lapply(colnames(Set.Train.ProbClass1.mtx), function(MM) do.call(&#39;rbind&#39;, lapply(rev(sort(unique(Set.Train.ProbClass1.mtx))), function(TS) { TP &lt;- sum(Set.Train.ProbClass1.mtx[,MM][Train.Label.vec==CLASS1] &gt; TS, na.rm=T) FP &lt;- sum(Set.Train.ProbClass1.mtx[,MM][Train.Label.vec==CLASS2] &gt; TS, na.rm=T) c(TS=TS, TP=TP, FP=FP)})) ) names(Set.Train.roc.mtx.lst) &lt;- colnames(Set.Train.ProbClass1.mtx) # Get auc Set.Train.auc.vec &lt;- sapply(1:ncol(Set.Train.ProbClass1.mtx), function(CC) auc(Train.Label.vec, Set.Train.ProbClass1.mtx[,CC])) ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases names(Set.Train.auc.vec) &lt;- colnames(Set.Train.ProbClass1.mtx) plot(x=range(do.call(&#39;c&#39;, lapply(Set.Train.roc.mtx.lst, function(LL) LL[,&#39;FP&#39;])))/sum(Train.Label.vec==CLASS2), y=range(do.call(&#39;c&#39;, lapply(Set.Train.roc.mtx.lst, function(LL) LL[,&#39;TP&#39;])))/sum(Train.Label.vec==CLASS1), xlab=&#39;FP&#39;, ylab=&#39;TP&#39;, type=&#39;n&#39;) for(II in 1:length(Set.Train.roc.mtx.lst)) lines(x=Set.Train.roc.mtx.lst[[II]][,&#39;FP&#39;]/sum(Train.Label.vec==CLASS2), y=Set.Train.roc.mtx.lst[[II]][,&#39;TP&#39;]/sum(Train.Label.vec==CLASS1), col=Model.col[names(Set.Train.roc.mtx.lst)[II]]) abline(0,1, col=&#39;grey&#39;) legend(&#39;bottomright&#39;, legend=paste(names(Set.Train.roc.mtx.lst), &#39;:&#39;, round(Set.Train.auc.vec[names(Set.Train.roc.mtx.lst)],3),sep=&#39;&#39;), col=Model.col[names(Set.Train.roc.mtx.lst)], lty=1) title(&#39;Model Performance on Train Set&#39;) 4.4.2 Out-of-sample Train: Average over Repeats ### BREAK CACHE suppressMessages(require(pROC)) # Get predicted probabilities Set.ModelFit.osProbClass1.mtx.lst &lt;- lapply(Set.ModelFit.lst, function(MF) { MF.pred.mtx &lt;- MF$pred Rep.vec &lt;- sapply(strsplit(MF$pred$Resample, split=&#39;\\\\.&#39;),&#39;[&#39;, 2) rowIndex.vec &lt;- sort(unique(MF$pred$rowIndex)) ProbClass1.mtx &lt;- do.call(&#39;cbind&#39;, lapply(split(MF$pred, Rep.vec), function(RepMFpred.frm) RepMFpred.frm[match(rowIndex.vec, RepMFpred.frm$rowIndex),CLASS1])) ProbClass1.mtx}) ################################# # Use average osProb ################################# Set.Train.mean_osProbClass1.mtx &lt;- do.call(&#39;cbind&#39;, lapply(Set.ModelFit.osProbClass1.mtx.lst, function(osProbClass1.mtx) apply(osProbClass1.mtx,1,mean))) rownames(Set.Train.mean_osProbClass1.mtx) &lt;- names(Train.Label.vec) # ROC Set.Train.mean_osProbClass1.roc.mtx.lst &lt;- lapply(colnames(Set.Train.mean_osProbClass1.mtx), function(MM) do.call(&#39;rbind&#39;, lapply(rev(sort(unique(Set.Train.mean_osProbClass1.mtx))), function(TS) { TP &lt;- sum(Set.Train.mean_osProbClass1.mtx[,MM][Train.Label.vec==CLASS1] &gt; TS, na.rm=T) FP &lt;- sum(Set.Train.mean_osProbClass1.mtx[,MM][Train.Label.vec==CLASS2] &gt; TS, na.rm=T) c(TS=TS, TP=TP, FP=FP)})) ) names(Set.Train.mean_osProbClass1.roc.mtx.lst) &lt;- colnames(Set.Train.mean_osProbClass1.mtx) # Get auc Set.Train.mean_osProbClass1.auc.vec &lt;- sapply(1:ncol(Set.Train.mean_osProbClass1.mtx), function(CC) auc(Train.Label.vec, Set.Train.mean_osProbClass1.mtx[,CC])) ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases names(Set.Train.mean_osProbClass1.auc.vec) &lt;- colnames(Set.Train.mean_osProbClass1.mtx) plot(x=range(do.call(&#39;c&#39;, lapply(Set.Train.mean_osProbClass1.roc.mtx.lst, function(LL) LL[,&#39;FP&#39;])))/sum(Train.Label.vec==CLASS2), y=range(do.call(&#39;c&#39;, lapply(Set.Train.mean_osProbClass1.roc.mtx.lst, function(LL) LL[,&#39;TP&#39;])))/sum(Train.Label.vec==CLASS1), xlab=&#39;FP&#39;, ylab=&#39;TP&#39;, type=&#39;n&#39;) for(II in 1:length(Set.Train.mean_osProbClass1.roc.mtx.lst)) lines(x=Set.Train.mean_osProbClass1.roc.mtx.lst[[II]][,&#39;FP&#39;]/sum(Train.Label.vec==CLASS2), y=Set.Train.mean_osProbClass1.roc.mtx.lst[[II]][,&#39;TP&#39;]/sum(Train.Label.vec==CLASS1), col=Model.col[names(Set.Train.roc.mtx.lst)[II]]) abline(0,1, col=&#39;grey&#39;) legend(&#39;bottomright&#39;, legend=paste(names(Set.Train.mean_osProbClass1.auc.vec), &#39;:&#39;, round(Set.Train.mean_osProbClass1.auc.vec,3),sep=&#39;&#39;), col=Model.col[names(Set.Train.mean_osProbClass1.auc.vec)], lty=1) title(&#39;Model Performance on Train Set - OS mean Prob(MSS)&#39;) 4.4.3 Out-of-sample Set.Train: Individual Repeats ### BREAK CACHE suppressMessages(require(pROC)) ################################# # Replot ROC keep individual rep osProbClass1 ################################# Set.Train.indiv_osProbClass1.mtx &lt;- do.call(&#39;cbind&#39;, lapply(names(Set.ModelFit.osProbClass1.mtx.lst), function(MODEL) { osProbClass1.mtx &lt;- Set.ModelFit.osProbClass1.mtx.lst[[MODEL]] colnames(osProbClass1.mtx) &lt;- paste(MODEL, colnames(osProbClass1.mtx),sep=&#39;.&#39;) osProbClass1.mtx})) rownames(Set.Train.indiv_osProbClass1.mtx) &lt;- names(Train.Label.vec) # ROC Set.Train.indiv_osProbClass1.roc.mtx.lst &lt;- lapply(colnames(Set.Train.indiv_osProbClass1.mtx), function(MM) do.call(&#39;rbind&#39;, lapply(rev(sort(unique(Set.Train.indiv_osProbClass1.mtx))), function(TS) { TP &lt;- sum(Set.Train.indiv_osProbClass1.mtx[,MM][Train.Label.vec==CLASS1] &gt; TS, na.rm=T) FP &lt;- sum(Set.Train.indiv_osProbClass1.mtx[,MM][Train.Label.vec==CLASS2] &gt; TS, na.rm=T) c(TS=TS, TP=TP, FP=FP)})) ) names(Set.Train.indiv_osProbClass1.roc.mtx.lst) &lt;- colnames(Set.Train.indiv_osProbClass1.mtx) # Get auc Set.Train.indiv_osProbClass1.auc.vec &lt;- sapply(1:ncol(Set.Train.indiv_osProbClass1.mtx), function(CC) auc(Train.Label.vec, Set.Train.indiv_osProbClass1.mtx[,CC])) ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases ## Setting levels: control = MSI, case = MSS ## Setting direction: controls &gt; cases names(Set.Train.indiv_osProbClass1.auc.vec) &lt;- colnames(Set.Train.indiv_osProbClass1.mtx) Set.Train.mean_indiv_osProbClass1.auc.vec &lt;- sapply(split(Set.Train.indiv_osProbClass1.auc.vec, sapply(strsplit(names(Set.Train.indiv_osProbClass1.auc.vec), split=&#39;\\\\.&#39;),&#39;[&#39;,1)), mean) plot(x=range(do.call(&#39;c&#39;, lapply(Set.Train.indiv_osProbClass1.roc.mtx.lst, function(LL) LL[,&#39;FP&#39;])))/sum(Train.Label.vec==CLASS2), y=range(do.call(&#39;c&#39;, lapply(Set.Train.indiv_osProbClass1.roc.mtx.lst, function(LL) LL[,&#39;TP&#39;])))/sum(Train.Label.vec==CLASS1), xlab=&#39;FP&#39;, ylab=&#39;TP&#39;, type=&#39;n&#39;) for(II in 1:length(Set.Train.indiv_osProbClass1.roc.mtx.lst)) lines(x=Set.Train.indiv_osProbClass1.roc.mtx.lst[[II]][,&#39;FP&#39;]/sum(Train.Label.vec==CLASS2), y=Set.Train.indiv_osProbClass1.roc.mtx.lst[[II]][,&#39;TP&#39;]/sum(Train.Label.vec==CLASS1), col=Model.col[sapply(strsplit(names(Set.Train.indiv_osProbClass1.roc.mtx.lst),&#39;\\\\.&#39;),&#39;[&#39;,1)[II]]) abline(0,1, col=&#39;grey&#39;) legend(&#39;bottomright&#39;, legend=paste(names(Set.Train.mean_indiv_osProbClass1.auc.vec), &#39;:&#39;, round(Set.Train.mean_indiv_osProbClass1.auc.vec,3),sep=&#39;&#39;), col=Model.col[names(Set.Train.mean_indiv_osProbClass1.auc.vec)],lty=1) title(&#39;Model Performance on Set.Train Set - indiv OS Prob(MSS)&#39;) # Boxplot individual aucs boxplot(split(Set.Train.indiv_osProbClass1.auc.vec, sub(&#39;Fit&#39;,&#39;&#39;, sapply(strsplit(names(Set.Train.indiv_osProbClass1.auc.vec), split=&#39;\\\\.&#39;),&#39;[&#39;,1)))) title(&#39;Distribution of AUC stats over CV reps&#39;) Set.Train.indiv_osProbClass1.auc.mtx &lt;- do.call(&#39;rbind&#39;, lapply(split(Set.Train.indiv_osProbClass1.auc.vec, sapply(strsplit(names(Set.Train.indiv_osProbClass1.auc.vec), split=&#39;\\\\.&#39;),&#39;[&#39;,1)), function(x) {Res=x; names(Res)&lt;- sapply(strsplit(names(Res),split=&#39;\\\\.&#39;),&#39;[&#39;,2);Res})) # This is redundant #kable(data.frame(Set.Train.indiv_osProbClass1.auc.mtx, #mean_os=Set.Train.mean_osProbClass1.auc.vec), ##Test = Test.auc.vec), #digits=2, format=&#39;html&#39;,align=&#39;c&#39;) 4.4.4 Compare Predicted Probabilities With the repeated CV fitting set-up, we can examine the distribution of out-of-sample predictions. This is useful to both characterize the mode of errors occuring in a given model - are the errors due to bias or variability? - as well as chracterizing samples - some samples may be mis-labelled or hard to classify correctly. ### BREAK CACHE if(sum(rownames(Set.Train.indiv_osProbClass1.mtx) != names(Train.Label.vec))) stop(&quot;Sample ordering problem.&quot;) # Reorder by Outcome row.o &lt;- order(Train.Label.vec, names(Train.Label.vec)) Col.Models.vec &lt;- sapply(strsplit(colnames(Set.Train.indiv_osProbClass1.mtx), split=&#39;\\\\.&#39;),&#39;[&#39;,1) par(mfrow=c(length(unique(Col.Models.vec)),1), mar=c(0,5,2,1), oma=c(5,0,1,0)) for(MOD in unique(Col.Models.vec)) { Mod.cols &lt;- which(Col.Models.vec==MOD) box.out &lt;- boxplot(t(Set.Train.indiv_osProbClass1.mtx[row.o,Mod.cols]), col=ifelse(Train.Label.vec[row.o]==CLASS1,3,2), xaxt=&#39;n&#39;, outline=F) title(MOD) } 4.5 Look at Variable Importance The caret package provides methods to extract variable importance through the varImp function. Here we will extract these assessments for each model and compare with genes which have been identified as associated with MSS status in other studies: BANERJEA [21] CROCE [22] JORISSEN [23] KOINUMA [24] KRUHOFFER [25] MORI [26] ### BREAK CACHE MSSMSI.GeneSets.frm &lt;- read.table(file=file.path(&#39;extData&#39;, &quot;ColonCancerGeneSets.tab&quot;), header=T, sep=&#39;\\t&#39;) GeneSets.lst &lt;- split(MSSMSI.GeneSets.frm$GeneSymbol, toupper(MSSMSI.GeneSets.frm$ListName)) GeneSets.lst &lt;- GeneSets.lst[c(&#39;BANERJEA&#39;,&#39;CROCE&#39;,&#39;JORISSEN&#39;, &#39;KOINUMA&#39;, &#39;KRUHOFFER&#39;,&#39;MORI&#39;)] ### BREAK CACHE suppressMessages(require(caret)) suppressMessages(require(gbm)) # Load gene to probe set map load(file=file.path(&#39;RData&#39;, &#39;GeneNameMap.vec&#39;)) # Will also need the inverted map GeneNameMap2.vec &lt;- names(GeneNameMap.vec) names(GeneNameMap2.vec) &lt;- GeneNameMap.vec Set.ModelFit.Top20.lst &lt;- lapply(setdiff(names(Set.ModelFit.lst), c(&quot;knnFit&quot;,&quot;sddaLDAFit&quot;,&quot;sddaQDAFit&quot;)), function(MOD) { #cat(MOD,&#39;\\n&#39;) FIT &lt;- Set.ModelFit.lst[[MOD]] if(is.element(CLASS1, colnames(varImp(FIT)$imp))) impVar.vec &lt;- varImp(FIT)$imp[,CLASS1] else impVar.vec &lt;- varImp(FIT)$imp$Overall top20.ndx &lt;- rev(order(impVar.vec))[1:20] varImp.vec &lt;- impVar.vec[top20.ndx] names(varImp.vec) &lt;- rownames(varImp(FIT)$imp)[top20.ndx] varImp.vec }) names(Set.ModelFit.Top20.lst) &lt;- setdiff(names(Set.ModelFit.lst), c(&quot;knnFit&quot;,&quot;sddaLDAFit&quot;,&quot;sddaQDAFit&quot;)) Top20.Name.vec &lt;- unique(do.call(&#39;c&#39;, lapply(Set.ModelFit.Top20.lst, function(VV) names(VV)))) Top20.ProbeId.vec &lt;- GeneNameMap.vec[Top20.Name.vec] # Put together in a matrix Top20.varImp.mtx &lt;- do.call(&#39;cbind&#39;, lapply(Set.ModelFit.Top20.lst, function(LL) LL[Top20.Name.vec])) rownames(Top20.varImp.mtx) &lt;- GeneNameMap.vec[Top20.Name.vec] Top20.varImp.mtx[is.na(Top20.varImp.mtx)] &lt;- 0 # Reoder by ovrall importance varImp.med.vec &lt;- apply(Top20.varImp.mtx,1,median) Top20.varImp.mtx &lt;- Top20.varImp.mtx[rev(order(varImp.med.vec)),] Top20.varImp.frm &lt;- data.frame(PROBEID=rownames(Top20.varImp.mtx), Gene=GeneNameMap2.vec[rownames(Top20.varImp.mtx)], round(Top20.varImp.mtx)) names(Top20.varImp.frm) &lt;- sub(&#39;Fit&#39;, &#39;&#39;, names(Top20.varImp.frm)) # Add geneset membership Top20Genes.vec &lt;- sapply(strsplit(Top20.varImp.frm$Gene, split=&#39;\\\\.&#39;),&#39;[&#39;,1) Top20GeneSetElements.frm &lt;- data.frame(do.call(&#39;cbind&#39;, lapply(GeneSets.lst, function(GS) ifelse(is.element(Top20Genes.vec, GS),&#39;Y&#39;,&#39;&#39;)))) colnames(Top20GeneSetElements.frm) &lt;- names(GeneSets.lst) Top20.varImp.frm &lt;- data.frame(Top20.varImp.frm, Top20GeneSetElements.frm) o.v &lt;- rev(order(apply(Top20.varImp.frm[,sub(&#39;Fit&#39;,&#39;&#39;,names(Set.ModelFit.Top20.lst))], 1, mean))) REPLACE &lt;- function() { knitr::kable(Top20.varImp.frm[o.v,], align=&#39;c&#39;, row.names=F, caption=&quot;Top 20 Features&quot;) %&gt;% kableExtra::kable_styling(full_width = F) } DT::datatable(Top20.varImp.frm[o.v,], rownames=F, caption=&quot;Top 20 Features&quot;) 4.6 Classification: Discussion 4.6.1 Nearest shrunken centroids does well The nearest shrunken centriod method described in Tibshirani et. al. [Tibshirani:2002aa] does very well both in terms of classification accuracy, computing time and simplicity of predictor. This has been our experience with many classification problems based on gene expression data. 4.6.2 Lack of agreement with literature gene sets The variable importance assessment shows that the genes which are deemed important in the clsssifiers the we fitted have little overlap with the gene sets previously identified as being associated with MSS status. Part of this lack of overlap is certanly due to the gene selection filter which we applied here for computing purposes. This not a problem if we just want to build a classifier which predicts well. It is somewhat of a problem if we want to use biology to validate our empirically determined models. In that respect, the lack of agreement of gene lists across analyses is always a problem to contend with. 4.6.3 Models could be better optimized Note that we did not attempt to optimize the model tuning parameters in any way and just used the default search grids for each model. While this may be a good choice on average, better performance could be obtained from some of the models by specifying a tuning parameter space which is better suited to the problem at hand. This requires a good understanding of each model and is beyond the scope of this vignette. References "],["clustering.html", "Section 5 Cluster Analysis 5.1 Bootstrap Aggregation of Clusters 5.2 t-SNE 5.3 Cluster Analysis: Discussion 5.4 Next Step: Look for subgroups in the MSS population - an open biological question.", " Section 5 Cluster Analysis In some cases one may be interested in discovering sugroups of samples within the sampled population which are more homogenious than the popolation as a whole. Verhaak et. al. [27] describe an analysis pipeline for cluster discovery consisting of the following steps: Data collection and integration Identification of gene-expression based sub-types by cluster analysis Marker gene signature identification Clustering validation and assessment of clinical significance In this section we will illustrate the cluster analysis step with two methods: Clustering based on partitioning around medoids (PAM) along with bootstrap aggregating (or Bagging). t-SNE The PAM algorithm for clustering is described in detail in Kaufman and Rousseeuw [28]. t-SNE is described in here, with some illustrations here. We will treat the MSS/MSI status of samples in our data set as unknown and see if the clustering algorithms can discover these hidden classes. A more interesting problem on the biological pint of view would be to try to discover truly unknown subgroups within the MSS and the MSI groups. The PAM Clustering Algorithm The PAM algorithm for clustering has some desirable attributes: the algorithm provides a way of estimating the number of clusters present in the data each point is assigned to a cluster and a measure of strength or confidence in the cluster assignment is provided for each individual observation cluster homogeneity can be assessed. Underlying the PAM algorithm is a notion of similarity or proximity. Basically, the PAM clustering algorithm sorts out a set of samples into subgroups such that within each subgroup, samples are more similar to other members of the subgroup than to samples assigned to other subgroups. To define the notion of proximity, suppose the gene expression indicators are stored in a matrix \\(X\\) with entries \\(x_{ij}\\) being the gene expression indicator on log base 2 scale for sample i gene j. Similarity among samples can be encapsulated by any of a number of measures of distance between the rows of the matrix \\(X\\). Some common choices are the Euclidean distance (average coordinate squared difference), the Manhattan distance (average coordinate absolute difference), and a suitably transformed measure of correlation (2 – Pearson correlation, for example). More generally, any weighted average of a coordinate specific measure of similarity can be used as a measure of sample similarity. It is important to point out that for any fixed choice of distance quite different clustering results can be obtained depending on the selection of genes, or probe sets, used to compute the sample to sample distances. This is important to note because it is often necessary to apply a screen to subset genes or probe sets at the outset. This is sometimes done to improve the computing efficiency of the analysis process. Our experience with these data shows that if no screening is applied and all 50K+ probe sets are used to compute sample similarity measures, the clustering procedures are unable to detect even obvious structure in the data – the distinction between MSS and MSI samples, for example. In this analysis, we applied a fairly aggressive screen based on the overall data set variability of expression indicators. It might be advisable to apply the variability constraint to within data set variability to avoid selecting probe sets that may have high overall variability due to between data set artifactual variation. This wasn’t done here. Having computed a distance or dissimilarity matrix, the PAM algorithm proceeds iteratively as follows: For fixed k (pre-specified number of clusters), select k representative samples arbitrarily. Each sample of the data set is assigned to the nearest medoid. Update medoids by minimizing the objective function Repeat steps 2-3 until there is no further change. To measure confidence in the cluster assignment for each sample, and to get a sense of cluster homogeneity, the notion of silhouette is used. The silhouette score is a normalized score: \\[s_i = \\frac{b_i – a_i}{max(a_i, b_i)}\\] where \\(a_i\\) = average distance between sample i and all other samples within the cluster i is assigned to, and \\(b_i\\) = minimum average distance between sample i and samples in other clusters. Individual clusters can be characterized by average silhouette width. The clustering or partitioning of a dataset can be summarized by the overall average silhouette width and his can be used to select the number of clusters, k, that best describes the structure in the data set. Bootstrap Aggregating of Clusters The application of bagging to clustering is discussed in Dudoit and Fridlyand [29]. Bootstrap aggregation entails performing the clustering analysis repeatedly on bootstrap subsets of samples – sample sets resulting from randomly selecting samples from the original set of samples. The clustering results for the bootstrap samples are then pooled, or aggregated, to produce the final clustering results. Two methods of aggregation are proposed in Dudoit and Fridlyand [29]. One method (BagClust1) allocates samples to clusters according to a voting scheme. The other method (BagClust2) uses the relative frequency across bootstrap samples with which sample pairs are clustered together as a new measure of distance which is then used by the PAM algorithm to cluster the samples. In our exploration we have found the two approaches to give rise to comparable results. We find the latter to be preferable as the distance matrix used for aggregating the bootstrap clustering results can be used like any other distance matrix to assess confidence in sample assignments and overall clustering homogeneity. Alternative methods of bagging clusters exist. buster is an R package which implements some form of bagging of hierarchical clustering results. Li [30] discusses a bagged clustering algorithm which is resistent to outliers and scalable. Clustering preliminaries - load data and set parameters. # Load Data loadObj(paste0(‘Train.’,SelGenes,‘.vec’), ‘SelGenes.vec’) load(file=file.path(‘RData’, ‘Train.Expr.mtx’)) Train.SelGenes.Expr.mtx &lt;- Train.Expr.mtx[, SelGenes.vec] rm(Train.Expr.mtx) load(file=file.path(‘RData’, ‘Train.Label.vec’)) load(file=file.path(‘RData’, ‘Train.DataSource.vec’)) #### Clustering Parmeters: - SelGenes = Top30pVarGenes - Distance Metric = euclidean - Standardize = TRUE ## PAM Clustering We start by performing a standard PAM cluster analysis of the data. To quantify the concordance between the discoverd clusters and the MSS/MSI labels, we can compute [Cohen&#39;s Kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa). ```r suppressMessages(require(stats)) suppressMessages(require(cluster)) Expr.mtx &lt;- Train.SelGenes.Expr.mtx # Standardize? if(STAND) Expr.mtx &lt;- scale(Expr.mtx) # Get dissimilarity {if(DM == &quot;1-pearson&quot;) Expr.dist &lt;- as.dist(1-cor(t(Expr.mtx))) else Expr.dist &lt;- daisy(Expr.mtx, DM) } # get ave sil width for range of K asw.vec &lt;- sapply(2:5, function(kk) pam(Expr.dist, diss=T, k=kk)$silinfo$avg.width) names(asw.vec) &lt;- paste0(&#39;K_&#39;, 2:5) print(round(asw.vec,3)) ## K_2 K_3 K_4 K_5 ## 0.020 0.012 0.010 0.004 PAM returns the correct number of clusters, if we are trying to recover the MSS/MSI subgroups. Let us look at what the clustering looks like. ###, results=&#39;hold&#39;} this r chunk option doesn&#39;t appear to work! suppressMessages(require(cluster)) BestK &lt;- as.numeric(sub(&#39;K_&#39;,&#39;&#39;, names(asw.vec)[which.max(asw.vec)])) # Make sure Expr.dist hasn&#39;t changed! Expr.pam &lt;- pam(Expr.dist, k=BestK, keep.diss=T) # Look at agreement with MSS label Pam.agree.tbl &lt;- table(Expr.pam$clustering, Train.Label.vec) # Compute kappa for display on figure # (Only makes sense for BestK==2) kappa.v &lt;- NA if(BestK == 2) kappa.v &lt;- getKappa(Expr.pam$clustering, Train.Label.vec) ## Loading required package: concord ####################################### # Visualization ####################################### old_par &lt;- par(mfrow=c(1,1), oma=c(8,1,5,1), #mar=c(4.2, 3.2, 1.1, 0.2), #bg=rgb(0,0,.606),fg=7,col.axis=7,col.lab=7,col.main=7, col.sub=7, cex.main=1.0,cex.lab=1.0,cex.axis=1.0) # DEBUG #save(Expr.pam, file=file.path(WRKDIR,&#39;Data&#39;, &#39;Expr.pam&#39;)) #THIS DOEST WORK ANY MORE SKIP &lt;- function() { plot(Expr.pam,main=&#39;&#39;) mtext2by2Tbl(Pam.agree.tbl) }# SKIP ################################### clusplot(Expr.pam, main=&#39;&#39;)###, pch=DataSource.pch) ### Recolor samples according to MSS/MSI status #xD &lt;- eval(Expr.pam$call[[2]]) xD &lt;- Expr.pam$diss x1 &lt;- cmdscale(xD, k=2, eig=T, add=T) if(x1$ac &lt; 0) x1 &lt;- cmdscale(xD, k=2, eig=T) #var.dec &lt;- x1$GOF[2] x1 &lt;- x1$points points(x1[,1][Train.Label.vec==&#39;MSI&#39;], x1[,2][Train.Label.vec==&#39;MSI&#39;], pch=&#39;.&#39;,cex=3, col=&#39;red&#39;) # add 2x2 tbale in margin mtext2by2Tbl(Pam.agree.tbl) title(paste(&#39;Red dot = MSI samples&#39;, &#39;\\nAve. Sil. Width = &#39;, round(Expr.pam$silinfo$avg.width,3), &#39; Kappa = &#39;, round(kappa.v,3))) par(old_par) Although the MSS and MSI somewhat colocate on the two dimensional projection of their gene expression vectors, one would be hard-pressed to make the case that clusters exist in these data. Next we’ll see if bootstrap aggregation helps. 5.1 Bootstrap Aggregation of Clusters Next we will implement the baaged clustering procedure, BagClust2: For a fixed number of clusters K: Initialize matrices \\(A_{nxn}\\) and \\(M_{nxn}\\) to zeroes. Repeat 2-4 N_BOOT times: 2. Form the $b^{th}$ bootstrap sample, Bt_Samp 3. Cluster Bt_Samp to obtain cluster labels Bt_Clust 4. For each pair of samples in Bt_Samp, s1, s2 - increment $M_{s1,s1}$ - increment $A_{s1,s1}$ if s1 and s2 co-cluster in the $b^{th}$ bootstarp sample. end repeat. Define a new dissimilarity matrix \\(D = 1 - A/M\\) Cluster samples on the basis of this dissimilarity matrix. suppressMessages(require(cluster)) suppressMessages(require(stats)) N_BOOT &lt;- 30 # initialize list to store BagCLust results SelGenes.BagClust.pam.lst &lt;- list() for(KK in 2:5) { # Initialize counting matrices A.mtx &lt;- matrix(0, ncol=length(Train.Label.vec), nrow=length(Train.Label.vec)) rownames(A.mtx) &lt;- names(Train.Label.vec) colnames(A.mtx) &lt;- names(Train.Label.vec) M.mtx &lt;- matrix(0, ncol=length(Train.Label.vec), nrow=length(Train.Label.vec)) rownames(M.mtx) &lt;- names(Train.Label.vec) colnames(M.mtx) &lt;- names(Train.Label.vec) # Repeat 2-4 B times # 2. Form the b_th bootstrap sample Bt_Samp = # sample(Samp.vec, size=N_Samp, replace=T) # 3. Cluster to obtain cluster labels Bt_Clust for samples in Bt_Samp # 4. For each pair of samples in Bt_Samp, s1, s2, # - increment M[s1,s2] # - increment A[s1,s1] if Bt_Clust[s1] == Bt_Clust[2] # end repeat for(BB in 1:N_BOOT){ cat(&#39;.&#39;) # 2. form the BS sample B_Samp &lt;- sample(names(Train.Label.vec), size=length(Train.Label.vec), replace=T) Expr.B.mtx &lt;- Expr.mtx[B_Samp,] # Get dissimilarity {if(DM == &quot;1-pearson&quot;) Expr.B.dist &lt;- as.dist(1-cor(t(Expr.B.mtx))) else Expr.B.dist &lt;- daisy(Expr.B.mtx, DM) } rm(Expr.B.mtx) # 3. cluster Expr.B.pam &lt;- pam(Expr.B.dist, k=KK) # 4. For each pair of samples in Bt_Samp, s1, s2, # - increment M[s1,s2] # - increment A[s1,s1] if Bt_Clust[s1] == Bt_Clust[2] B_Samp.lst &lt;- unique(B_Samp) M.mtx[B_Samp.lst, B_Samp.lst] &lt;- M.mtx[B_Samp.lst, B_Samp.lst] + 1 # patition in list B_Samp.Cluster &lt;- Expr.B.pam$clustering[B_Samp.lst] B_Samp.Clust.lst &lt;- split(B_Samp.lst, B_Samp.Cluster) for(LL in 1:length(B_Samp.Clust.lst)) { Clust.Samp &lt;- B_Samp.Clust.lst[[LL]] A.mtx[Clust.Samp, Clust.Samp] &lt;- A.mtx[Clust.Samp, Clust.Samp] + 1 } } # end BB loop cat(&#39;\\n&#39;) # 5. Define new dissimilariy matirx D = 1 - A/M D.mtx &lt;- A.mtx/M.mtx D.mtx[is.na(D.mtx)] &lt;- 0 # 6. Cluster observations on the basis of this dissimilarity matrix D.dist &lt;- as.dist(1-D.mtx) SelGenes.BagClust.pam.lst[[paste0(&#39;K_&#39;,KK)]] &lt;- pam(D.dist, k=KK, keep.diss=T) cat(&#39;KK =&#39;,KK, &#39; sil.avg.width =&#39;, SelGenes.BagClust.pam.lst[[paste0(&#39;K_&#39;,KK)]]$silinfo$avg.width, &#39;\\n&#39;) }#for(KK ## .............................. ## KK = 2 sil.avg.width = 0.2822617 ## .............................. ## KK = 3 sil.avg.width = 0.2749272 ## .............................. ## KK = 4 sil.avg.width = 0.2862272 ## .............................. ## KK = 5 sil.avg.width = 0.1644449 ## save saveObj(paste0(SelGenes, &#39;.BagClust.pam.lst&#39;), &#39;SelGenes.BagClust.pam.lst&#39;) suppressMessages(require(cluster)) loadObj(paste0(SelGenes, &#39;.BagClust.pam.lst&#39;), &#39;SelGenes.BagClust.pam.lst&#39;) # Get best K model BagClustBestK &lt;- names(SelGenes.BagClust.pam.lst)[which.max(sapply(SelGenes.BagClust.pam.lst, function(x) x$silinfo$avg.width))] BagClustBestK.pam &lt;- SelGenes.BagClust.pam.lst[[BagClustBestK]] # Get agreement table BagClustBestK.pam.agree.tbl &lt;- table(BagClustBestK.pam$clustering, Train.Label.vec) #Compute kappa for display on figure # (Only makes sense for KK==2) if(BagClustBestK == paste0(&#39;K_&#39;,2)) kappa.v &lt;- getKappa(BagClustBestK.pam$clustering, Train.Label.vec) #cat(&#39;kappa =&#39;, kappa.v, &#39;\\n&#39;) ####################################### # Visualization ####################################### old_par &lt;- par(mfrow=c(1,1), oma=c(8,1,5,1), #mar=c(4.2, 3.2, 1.1, 0.2), #bg=rgb(0,0,.606),fg=7,col.axis=7,col.lab=7,col.main=7, col.sub=7, cex.main=1.0,cex.lab=1.0,cex.axis=1.0) #plot(BagClustBestK.pam,main=&#39;&#39;) clusplot(BagClustBestK.pam, main=&#39;&#39;)###, pch=DataSource.pch) ### Recolor samples according to MSS/MSI status #xD &lt;- eval(BagClustBestK.pam$call[[2]]) xD &lt;- BagClustBestK.pam$diss x1 &lt;- cmdscale(xD, k=2, eig=T, add=T) if(x1$ac &lt; 0) x1 &lt;- cmdscale(xD, k=2, eig=T) #var.dec &lt;- x1$GOF[2] x1 &lt;- x1$points points(x1[,1][Train.Label.vec==&#39;MSI&#39;], x1[,2][Train.Label.vec==&#39;MSI&#39;], pch=&#39;.&#39;,cex=3, col=&#39;red&#39;) # add 2x2 tbale in margin mtext2by2Tbl(BagClustBestK.pam.agree.tbl) title(paste(&#39;Red dot = MSI samples&#39;, &#39;\\nAve. Sil. Width = &#39;, round(BagClustBestK.pam$silinfo$avg.width,3), &#39; Kappa = &#39;, round(kappa.v,3))) par(old_par) The groupings discovered by BagClust2 are essentially the same as those discovered by the PAM cluster analysis. The main effect of the bagging is to reduce noise and provide better separation between groups, which greatly increases our confidence in the groupings. Next we look at the application of t-SNE algorithm to this problem. 5.2 t-SNE t-SNE is a dimensionality reduction techique that is particularly well suited for the visualization of high-dimensional datasets. Here we will use the t-SNE algorithm to reduce the gene expression data martrix to a few dimensions. These will then be used as inputs to some clustering algorithm for the purpose of discovering subgroups in the data. Here we will assess the benefit of the t-SNE embedding by repeating the cluster analysis above - PAM and BagClust2 clustering - using the t-SNE embedding as input instead of the scales expression matrix. # Load Data loadObj(paste0(&#39;Train.&#39;,SelGenes,&#39;.vec&#39;), &#39;SelGenes.vec&#39;) load(file=file.path(&#39;RData&#39;, &#39;Train.Expr.mtx&#39;)) Train.SelGenes.Expr.mtx &lt;- Train.Expr.mtx[, SelGenes.vec] rm(Train.Expr.mtx) load(file=file.path(&#39;RData&#39;, &#39;Train.Label.vec&#39;)) load(file=file.path(&#39;RData&#39;, &#39;Train.DataSource.vec&#39;)) suppressMessages(require(Rtsne)) PERP &lt;- 50 SelGenes.tsne.lst &lt;- list() #for(DD in c(2,5,10)) for(DD in c(2, 3)) SelGenes.tsne.lst[[paste0(&#39;D_&#39;, DD)]] &lt;- Rtsne(Train.SelGenes.Expr.mtx, check_duplicates=FALSE, pca=TRUE, pca_center=T, pca_scale=T, perplexity=PERP, theta=0.0, dims=DD, max_iter = 5000) saveObj(paste0(SelGenes, &#39;.tsne.lst&#39;), &#39;SelGenes.tsne.lst&#39;) 5.2.1 t-SNE + PAM suppressMessages(require(stats)) suppressMessages(require(cluster)) loadObj(paste0(SelGenes, &#39;.tsne.lst&#39;), &#39;SelGenes.tsne.lst&#39;) tsne.asw.mtx &lt;- do.call(&#39;rbind&#39;, lapply(names(SelGenes.tsne.lst), function(DD) { Expr.mtx &lt;- SelGenes.tsne.lst[[DD]]$Y # Standardize? #if(STAND) Expr.mtx &lt;- scale(Expr.mtx) # Get dissimilarity {if(DM == &quot;1-pearson&quot;) Expr.dist &lt;- as.dist(1-cor(t(Expr.mtx))) else Expr.dist &lt;- daisy(Expr.mtx, DM) } # get ave sil width for range of K asw.vec &lt;- sapply(2:5, function(kk) pam(Expr.dist, diss=T, k=kk)$silinfo$avg.width) names(asw.vec) &lt;- paste0(&#39;K_&#39;, 2:5) asw.vec})) rownames(tsne.asw.mtx) &lt;- names(SelGenes.tsne.lst) print(round(tsne.asw.mtx,3)) ## K_2 K_3 K_4 K_5 ## D_2 0.427 0.394 0.391 0.367 ## D_3 0.346 0.329 0.300 0.287 PAM clustering based on t-SNE embeddings of various dimensions does not return allow us to discriminate among the number of clusters based on average silhouette width. Let’s see what the 2-D scatters look like as well as the PAM clustering into two groups. suppressMessages(require(cluster)) loadObj(paste0(SelGenes, &#39;.tsne.lst&#39;), &#39;SelGenes.tsne.lst&#39;) for(DD in names(SelGenes.tsne.lst)){ Expr.mtx &lt;- SelGenes.tsne.lst[[DD]]$Y # Standardize? #if(STAND) Expr.mtx &lt;- scale(Expr.mtx) # Get dissimilarity {if(DM == &quot;1-pearson&quot;) Expr.dist &lt;- as.dist(1-cor(t(Expr.mtx))) else Expr.dist &lt;- daisy(Expr.mtx, DM) } #for(KK in 2:5) { KK &lt;- 2 Expr.pam &lt;- pam(Expr.dist, diss=T, k=KK, keep.diss=T) Pam.agree.tbl &lt;- table(Expr.pam$clustering, Train.Label.vec) kappa.v &lt;- getKappa(Expr.pam$clustering, Train.Label.vec) ####################################### # Visualization ####################################### old_par &lt;- par(mfrow=c(1,1), oma=c(8,1,5,1), #mar=c(4.2, 3.2, 1.1, 0.2), #bg=rgb(0,0,.606),fg=7,col.axis=7,col.lab=7,col.main=7, col.sub=7, cex.main=1.0,cex.lab=1.0,cex.axis=1.0) #plot(Expr.pam,main=&#39;&#39;) ################################### clusplot(Expr.pam, main=&#39;&#39;)###, pch=DataSource.pch) ### Recolor samples according to MSS/MSI status #xD &lt;- eval(Expr.pam$call[[2]]) xD &lt;- Expr.pam$diss x1 &lt;- cmdscale(xD, k=2, eig=T, add=T) if(x1$ac &lt; 0) x1 &lt;- cmdscale(xD, k=2, eig=T) #var.dec &lt;- x1$GOF[2] x1 &lt;- x1$points points(x1[,1][Train.Label.vec==&#39;MSI&#39;], x1[,2][Train.Label.vec==&#39;MSI&#39;], pch=&#39;.&#39;,cex=3, col=&#39;red&#39;) # add 2x2 tbale in margin mtext2by2Tbl(Pam.agree.tbl) title(paste(&#39;Dim =&#39;, DD, &#39; - Red dot = MSI samples&#39;, &#39;\\nAve. Sil. Width = &#39;, round(Expr.pam$silinfo$avg.width,3), &#39; Kappa = &#39;, round(kappa.v,3))) par(old_par) }#for(DD PAM applied to t-SNE embeddings produce larger silhuoette profiles than PAM applied to the gene expression data. Let’s see how bagging helps here. 5.2.2 t-SNE + BagClust2 An important parameter in the application of t-SNE to clustering is the number of dimensions used in the t-SNE embedding. We will assess the effect of this parameter here, keeping the specified number of clusters fixed at k=2 while varying the number of embedding dimensions. suppressMessages(require(cluster)) suppressMessages(require(stats)) KK &lt;- 2 loadObj(paste0(SelGenes, &#39;.tsne.lst&#39;), &#39;SelGenes.tsne.lst&#39;) load(file=file.path(&#39;RData&#39;, &#39;Train.Label.vec&#39;)) N_BOOT &lt;- 30 # initialize list to store BagCLust results SelGenes.tsneBagClust.pam.lst &lt;- list() for(DD in names(SelGenes.tsne.lst)){ Expr.mtx &lt;- SelGenes.tsne.lst[[DD]]$Y # Add rownames (assuming order is correct!) rownames(Expr.mtx) &lt;- names(Train.Label.vec) # Initialize counting matrices A.mtx &lt;- matrix(0, ncol=length(Train.Label.vec), nrow=length(Train.Label.vec)) rownames(A.mtx) &lt;- names(Train.Label.vec) colnames(A.mtx) &lt;- names(Train.Label.vec) M.mtx &lt;- matrix(0, ncol=length(Train.Label.vec), nrow=length(Train.Label.vec)) rownames(M.mtx) &lt;- names(Train.Label.vec) colnames(M.mtx) &lt;- names(Train.Label.vec) # Repeat 2-4 B times # 2. Form the b_th bootstrap sample Bt_Samp = # sample(Samp.vec, size=N_Samp, replace=T) # 3. Cluster to obtain cluster labels Bt_Clust for samples in Bt_Samp # 4. For each pair of samples in Bt_Samp, s1, s2, # - increment M[s1,s2] # - increment A[s1,s1] if Bt_Clust[s1] == Bt_Clust[2] # end repeat for(BB in 1:N_BOOT){ cat(&#39;.&#39;) # 2. form the BS sample B_Samp &lt;- sample(names(Train.Label.vec), size=length(Train.Label.vec), replace=T) Expr.B.mtx &lt;- Expr.mtx[B_Samp,] # Get dissimilarity {if(DM == &quot;1-pearson&quot;) Expr.B.dist &lt;- as.dist(1-cor(t(Expr.B.mtx))) else Expr.B.dist &lt;- daisy(Expr.B.mtx, DM) } rm(Expr.B.mtx) # 3. cluster Expr.B.pam &lt;- pam(Expr.B.dist, k=KK) # 4. For each pair of samples in Bt_Samp, s1, s2, # - increment M[s1,s2] # - increment A[s1,s1] if Bt_Clust[s1] == Bt_Clust[2] B_Samp.lst &lt;- unique(B_Samp) M.mtx[B_Samp.lst, B_Samp.lst] &lt;- M.mtx[B_Samp.lst, B_Samp.lst] + 1 # patition in list B_Samp.Cluster &lt;- Expr.B.pam$clustering[B_Samp.lst] B_Samp.Clust.lst &lt;- split(B_Samp.lst, B_Samp.Cluster) for(LL in 1:length(B_Samp.Clust.lst)) { Clust.Samp &lt;- B_Samp.Clust.lst[[LL]] A.mtx[Clust.Samp, Clust.Samp] &lt;- A.mtx[Clust.Samp, Clust.Samp] + 1 } } # end BB loop cat(&#39;\\n&#39;) # 5. Define new dissimilariy matirx D = 1 - A/M D.mtx &lt;- A.mtx/M.mtx D.mtx[is.na(D.mtx)] &lt;- 0 # 6. Cluster observations on the basis of this dissimilarity matrix D.dist &lt;- as.dist(1-D.mtx) SelGenes.tsneBagClust.pam.lst[[DD]] &lt;- pam(D.dist, k=KK, keep.diss=T) cat(&#39;DD =&#39;,DD, &#39; sil.avg.width =&#39;, SelGenes.tsneBagClust.pam.lst[[DD]]$silinfo$avg.width, &#39;\\n&#39;) }#for(DD ## .............................. ## DD = D_2 sil.avg.width = 0.9368571 ## .............................. ## DD = D_3 sil.avg.width = 0.8470974 ## save saveObj(paste0(SelGenes, &#39;.tsneBagClust.pam.lst&#39;), &#39;SelGenes.tsneBagClust.pam.lst&#39;) suppressMessages(require(cluster)) loadObj(paste0(SelGenes, &#39;.tsneBagClust.pam.lst&#39;), &#39;SelGenes.tsneBagClust.pam.lst&#39;) load(file=file.path(&#39;RData&#39;, &#39;Train.Label.vec&#39;)) # Get best K model BagClustBestD &lt;- names(SelGenes.tsneBagClust.pam.lst)[which.max(sapply(SelGenes.tsneBagClust.pam.lst, function(x) x$silinfo$avg.width))] BagClustBestD.pam &lt;- SelGenes.tsneBagClust.pam.lst[[BagClustBestD]] # Get agreement table BagClustBestD.pam.agree.tbl &lt;- table(BagClustBestD.pam$clustering, Train.Label.vec) #Compute kappa for display on figure # (Only makes sense for KK==2) kappa.v &lt;- getKappa(BagClustBestD.pam$clustering, Train.Label.vec) #cat(&#39;kappa =&#39;, kappa.v, &#39;\\n&#39;) ####################################### # Visualization ####################################### old_par &lt;- par(mfrow=c(1,1), oma=c(8,1,5,1), #mar=c(4.2, 3.2, 1.1, 0.2), #bg=rgb(0,0,.606),fg=7,col.axis=7,col.lab=7,col.main=7, col.sub=7, cex.main=1.0,cex.lab=1.0,cex.axis=1.0) #plot(BagClustBestD.pam,main=&#39;&#39;) ################################### clusplot(BagClustBestD.pam, main=&#39;&#39;)###, pch=DataSource.pch) ### Recolor samples according to MSS/MSI status #xD &lt;- eval(BagClustBestD.pam$call[[2]]) xD &lt;- BagClustBestD.pam$diss x1 &lt;- cmdscale(xD, k=2, eig=T, add=T) if(x1$ac &lt; 0) x1 &lt;- cmdscale(xD, k=2, eig=T) #var.dec &lt;- x1$GOF[2] x1 &lt;- x1$points points(x1[,1][Train.Label.vec==&#39;MSI&#39;], x1[,2][Train.Label.vec==&#39;MSI&#39;], pch=&#39;.&#39;,cex=3, col=&#39;red&#39;) # add 2x2 tbale in margin mtext2by2Tbl(BagClustBestD.pam.agree.tbl) title(paste(&#39;Dim =&#39;, BagClustBestD, &#39; - Red dot = MSI samples&#39;, &#39;\\nAve. Sil. Width = &#39;, round(BagClustBestD.pam$silinfo$avg.width,3), &#39; Kappa = &#39;, round(kappa.v,3))) par(old_par) 5.3 Cluster Analysis: Discussion The PAM algorithm does a decent job at “discovering” the MSS vs MSI grouping in an unsupervised analysis of the gene expression vectors. In this analysis, we restricted the analysis to the Top30pVarGenes gene set. A different gene selection might give rise to sligthly different results. Bootstrap aggregation of PAM clustering results leads to cleaner separation among the groups but very little difference in the allocation of samples to groups. This is as expected. Bagging in thise case is a noise reduction measure. Applying the t-SNE algorithm to extract low dimensional embeddings of the gene expression vectors and applying the PAM algorthim to these data leads to some improved clustering results: separation of the groups is larger than is the results of applying PAM directly to the gene expression data. Bootstrap aggregation of results of applying PAM clustering to t-SNE embeddings leads to improved separation between groups and a slight improvement in the coherence between clusters and MSS vs MSI labels. It appears that both t-SNE embeddings and bootstrap aggregation are helpful in producing better separated clusters in this context. We have only skimmed the surface of cluster analysis of gene expression data in this vignette. We did not look at the propensity of the different approaches to produce fals positive results - the appearance of clusters when non exist. We also did not investigate the effect of the choice of value for the many parameters that can be adjusted in these analyses. In any given situation, the sensitivity of results produced by a method to changes in parameter settings or slight perturbations of the data is an important part of cluster analysis. 5.4 Next Step: Look for subgroups in the MSS population - an open biological question. References "],["summary.html", "Section 6 Results Summary", " Section 6 Results Summary "],["references.html", "References", " References "]]
